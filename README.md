# ğŸ“˜ DocumentaÃ§Ã£o: Serverless RAG Chatbot (MVP)

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral e Stack TecnolÃ³gico](#1-visÃ£o-geral-e-stack-tecnolÃ³gico)
2. [Registro de DecisÃµes e DÃºvidas](#2-registro-de-decisÃµes-e-dÃºvidas-architecture-decision-record)
3. [Fluxo de Trabalho Git](#3-fluxo-de-trabalho-git-padrÃ£o-promiles)
4. [Roteiro de ImplementaÃ§Ã£o](#4-roteiro-de-implementaÃ§Ã£o-roadmap)
5. [ImplementaÃ§Ã£o TÃ©cnica de ReferÃªncia](#5-implementaÃ§Ã£o-tÃ©cnica-de-referÃªncia)
6. [OtimizaÃ§Ã£o de Bundle: API-Only vs Full-Stack](#6-ğŸ“¦-otimizaÃ§Ã£o-de-bundle-api-only-vs-full-stack)
7. [Estrutura do Projeto](#7-estrutura-do-projeto)
8. [Como ComeÃ§ar](#8-como-comeÃ§ar)

---

## 1. VisÃ£o Geral e Stack TecnolÃ³gico

O objetivo Ã© criar um Chatbot de InteligÃªncia Artificial que responda perguntas baseadas em um documento PDF privado (RAG), com interface reativa e respostas via streaming.

### Frontend (AplicaÃ§Ã£o Web)
* **Framework:** **Nuxt 3** (Vue.js + TailwindCSS)
* **Gerenciamento de SessÃ£o:** **LocalStorage** (Navegador do Cliente)
* **ComunicaÃ§Ã£o:** `fetch` nativo com leitura de `ReadableStream`

### Backend (Serverless API)
* **Framework:** **Nuxt Nitro** (Server Routes - `/server/api`)
* **Runtime:** Node.js rodando em **AWS Lambda**
* **Acesso:** **Function URL** (Endpoint HTTPS direto, sem API Gateway complexo)
* **Streaming:** `InvokeMode: RESPONSE_STREAM`
* **Endpoints:**
    * `/api/ingest` - Upload e processamento de PDFs 
    * `/api/chat` - ConversaÃ§Ã£o com o chatbot (streaming de resposta)

### InteligÃªncia & Dados
* **LLM:** OpenAI `gpt-5-nano`
* **OrquestraÃ§Ã£o:** **LangChain.js** (Core & Community)
* **Banco de Conhecimento:** **Qdrant** (Vector Database)
    * *FunÃ§Ã£o:* Armazenar o PDF (chunks) e transformado em vetores

---

## 2. ğŸ§  Registro de DecisÃµes e DÃºvidas (Architecture Decision Record)

Esta seÃ§Ã£o detalha as dÃºvidas levantadas durante o planejamento e a soluÃ§Ã£o final adotada.

### 2.1. MemÃ³ria do Chat: Redis vs. LocalStorage vs. In-Memory
* **DÃºvida Levantada:** *"Devo usar Redis para o histÃ³rico? Posso usar variÃ¡vel global em Python/Node? O Qdrant salva o histÃ³rico? Banco nÃ£o relacional / relacional funciona aqui?"*
* **AnÃ¡lise:**
    * *Qdrant:* NÃ£o serve para histÃ³rico de conversa, serve apenas para conhecimento (PDF)
    * *In-Memory (variÃ¡vel global):* ImpossÃ­vel em AWS Lambda, pois a funÃ§Ã£o "morre" apÃ³s o uso (Stateless). VariÃ¡vel global Node seria perdida entre invocaÃ§Ãµes
    * *Banco Relacional (PostgreSQL, MySQL):* Funcionaria tecnicamente, mas Ã© "overkill" para MVP. Exige RDS (custo), gerenciamento de conexÃµes e esquema de dados
    * *Banco NÃ£o Relacional (DynamoDB, MongoDB):* TambÃ©m funcionaria, mas adiciona complexidade de queries e custo. DynamoDB seria serverless, mas precisa gerenciar TTL e cleanup
    * *Redis:* Seria a soluÃ§Ã£o robusta padrÃ£o para cache de sessÃ£o, mas adiciona custo (instÃ¢ncia paga) e complexidade de infraestrutura (VPC) desnecessÃ¡ria para um MVP
* **DecisÃ£o Final:** **Frontend-Driven History**
    * O Frontend guarda o array de mensagens no LocalStorage
    * A cada nova pergunta, o Frontend envia o **histÃ³rico completo** no `body` da requisiÃ§Ã£o
    * O Backend recebe tudo que precisa no request, processa e responde (stateless)
    * O Backend **nÃ£o armazena** e **nÃ£o busca** histÃ³rico em nenhum banco de dados
    * *IdentificaÃ§Ã£o (`x-user-id`):* O Front gera um UUID apenas para correlacionar logs/analytics. O backend **nÃ£o usa esse ID para buscar dados** (nÃ£o hÃ¡ dados armazenados para buscar!), apenas para logging/tracking. Ã‰ como um "nÃºmero de protocolo" - serve para rastrear requisiÃ§Ãµes nos logs, nÃ£o para recuperar histÃ³rico. Cada request Ã© independente e autocontido.

**Por que Frontend-Driven?**
- âœ… **Simples:** Sem infraestrutura extra (Redis, DynamoDB, etc)
- âœ… **EscalÃ¡vel:** Lambda Ã© stateless, mÃºltiplos usuÃ¡rios nÃ£o se interferem
- âœ… **Privado:** UsuÃ¡rio controla seus dados (localStorage local)
- âœ… **EconÃ´mico:** Zero custo de persistÃªncia para MVP
- âŒ NÃ£o persiste apÃ³s limpar dados do navegador (mas Ã© MVP)

### 2.2. ComunicaÃ§Ã£o: WebSockets vs. HTTP Streaming
* **DÃºvida Levantada:** *"Como fazer a conexÃ£o cliente servidor para esse projeto sabendo do lambda (sobe e morre)? Preciso de WebSockets? Http padrÃ£o resolve? Lambda suporta Streaming de respostas?"*
* **AnÃ¡lise:**
    * *WebSockets:* Em arquitetura Serverless, exigem API Gateway V2 e gerenciamento manual de conexÃµes (`@connections`) no DynamoDB. Muito complexo para uma via de mÃ£o Ãºnica (Bot respondendo)
    * *API Gateway PadrÃ£o:* Faz buffer da resposta (espera tudo ficar pronto para enviar), matando o efeito de digitaÃ§Ã£o
* **DecisÃ£o Final:** **Lambda Function URL com Response Streaming**
    * Usaremos o modo `RESPONSE_STREAM` nativo da Lambda
    * Isso permite enviar chunks de texto via HTTP padrÃ£o assim que o LangChain os gera

### 2.3. Framework Backend: Nuxt Nitro vs. Express.js
* **DÃºvida Levantada:** *"Por que usar Nitro? Existem outras alternativas como Express?"*
* **AnÃ¡lise:**
    * *Express:* Pesado, lento para iniciar (*cold start*) e nÃ£o otimizado para Serverless moderno
    * *Nitro:* Ã‰ o motor nativo do Nuxt. Permite escrever a API dentro da mesma pasta do projeto Frontend, compartilha a configuraÃ§Ã£o de build e compila para um pacote minÃºsculo otimizado para Lambda
* **DecisÃ£o Final:** **Nuxt Nitro**. Pela simplicidade de manter um Ãºnico repositÃ³rio (Monorepo implÃ­cito) e performance

### 2.4. EstratÃ©gia de RAG: Vetorial vs. Contexto Bruto
* **DÃºvida Levantada:** *"Por que um banco vetorial? Por que nÃ£o extrair o JSON do texto e jogar no prompt?"*
* **AnÃ¡lise:**
    * *Contexto Bruto:* Jogar um PDF inteiro no prompt estoura o limite de tokens do modelo e custa caro
* **DecisÃ£o Final:** **Qdrant (Vetorial)**
    * Usaremos o LangChain para dividir o PDF em pedaÃ§os
    * O Qdrant busca semanticamente apenas os trechos relevantes Ã  pergunta
    * A IA recebe apenas o necessÃ¡rio para responder

### 2.5. OrquestraÃ§Ã£o: LangChain vs. "Na MÃ£o"
* **DÃºvida Levantada:** *"Vamos usar LangChain ou chamar a OpenAI direto? LangChain nÃ£o Ã© complexo para streaming?"*
* **AnÃ¡lise:**
    * O LangChain antigo era complexo. O novo (LCEL - LangChain Expression Language) simplificou o streaming com o mÃ©todo `.stream()`
    * Fazer a ingestÃ£o do PDF (Splitters + Embeddings) "na mÃ£o" Ã© muito trabalhoso
* **DecisÃ£o Final:** **HÃ­brido/LangChain**
    * Usaremos LangChain para processar o PDF
    * Usaremos LangChain para o Chat, aproveitando a integraÃ§Ã£o nativa de streaming

### 2.6. Como o Qdrant Entende SemÃ¢ntica e Busca Vetores

* **DÃºvida Levantada:** *"Como o Qdrant entende a semÃ¢ntica? Como ele decide quais vetores enviar?"*

* **Resposta:** O Qdrant **nÃ£o interpreta texto diretamente**. Ele compara **vetores (embeddings)** que representam o significado semÃ¢ntico do texto.

#### ğŸ”„ Fluxo Completo de Busca SemÃ¢ntica

**Fase 1: IngestÃ£o (Salvar no Qdrant)**

```
1. Texto original do chunk:
   "Thiago Silva - Engenheiro de Software. ExperiÃªncia em FastAPI..."

2. OpenAI Embeddings transforma texto â†’ vetor:
   await embeddings.embedDocuments([chunk])
   â†’ [0.23, -0.45, 0.67, 0.12, ... 1536 nÃºmeros]

3. Salva no Qdrant:
   {
     id: uuid(),
     vector: [0.23, -0.45, 0.67, ...],  // â† Vetor (semÃ¢ntica)
     payload: {
       text: "Thiago Silva - Engenheiro..."  // â† Texto original
     }
   }
```

**Fase 2: Busca (Quando vocÃª pergunta)**

```
1. Sua pergunta:
   "sobre oq ele Ã©?"

2. Pergunta vira vetor (MESMO processo):
   await embeddings.embedQuery(question)
   â†’ [0.34, 0.12, -0.78, 0.89, ... 1536 nÃºmeros]

3. Qdrant compara vetores:
   await qdrant.search({
     vector: queryVector,  // â† Vetor da pergunta
     limit: 4              // â† Top 4 mais similares
   })
```

#### ğŸ“Š Como o Qdrant Compara Vetores

O Qdrant calcula a **similaridade** entre vetores usando **distÃ¢ncia cosseno** (ou outra mÃ©trica configurada).

**Exemplo Visual (Simplificado):**

Imagine vetores de 3 dimensÃµes (na prÃ¡tica sÃ£o 1536):

```
Chunk 1: "Engenheiro de Software"
Vetor: [0.8, 0.6, 0.2]

Chunk 2: "ExperiÃªncia em FastAPI"
Vetor: [0.3, 0.9, 0.1]

Chunk 3: "FormaÃ§Ã£o PUC-Campinas"
Vetor: [0.1, 0.2, 0.9]

Pergunta: "qual a profissÃ£o dele?"
Vetor: [0.7, 0.5, 0.3]
```

O Qdrant calcula a similaridade (distÃ¢ncia cosseno):

```
Similaridade(Pergunta, Chunk 1) = 0.95  â­ (muito similar)
Similaridade(Pergunta, Chunk 2) = 0.65  (pouco similar)
Similaridade(Pergunta, Chunk 3) = 0.25  (nÃ£o similar)
```

**Resultado:** Retorna Chunk 1 (mais similar).

#### ğŸ§  Por Que Funciona Semanticamente?

Os **embeddings da OpenAI** capturam **significado**, nÃ£o apenas palavras.

**Exemplo:**

```
Texto 1: "Ele Ã© programador"
Texto 2: "Ele trabalha como desenvolvedor"
Texto 3: "Ele gosta de futebol"
```

Mesmo com palavras diferentes, os vetores de "programador" e "desenvolvedor" ficam **prÃ³ximos** no espaÃ§o vetorial, enquanto "futebol" fica **distante**.

#### ğŸ”¢ MÃ©trica de DistÃ¢ncia (Cosine Similarity)

No seu cÃ³digo, a collection foi criada com:

```typescript
distance: 'Cosine'  // DistÃ¢ncia cosseno
```

**Como funciona:**

```
Similaridade = cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)

Onde:
- A = vetor da pergunta
- B = vetor do chunk
- Î¸ = Ã¢ngulo entre os vetores
- Resultado: 0.0 a 1.0 (1.0 = idÃªntico, 0.0 = completamente diferente)
```

#### ğŸ“ Exemplo PrÃ¡tico Completo

```
Pergunta: "quais tecnologias ele usa?"

1. Pergunta â†’ Vetor: [0.89, -0.23, 0.45, ...]

2. Qdrant compara com todos os chunks:
   Chunk A: "FastAPI, React, Next.js" 
     â†’ [0.87, -0.25, 0.43, ...] 
     â†’ Score: 0.92 â­
   
   Chunk B: "FormaÃ§Ã£o PUC-Campinas" 
     â†’ [0.12, 0.78, -0.34, ...] 
     â†’ Score: 0.31
   
   Chunk C: "ExperiÃªncia em IoT" 
     â†’ [0.65, -0.12, 0.78, ...] 
     â†’ Score: 0.68 â­
   
   Chunk D: "Data de nascimento: 1995" 
     â†’ [0.01, 0.02, 0.03, ...] 
     â†’ Score: 0.05

3. Retorna top 4 (ou menos se houver menos chunks):
   - Chunk A (Score: 0.92) âœ…
   - Chunk C (Score: 0.68) âœ…
   - Chunk B (Score: 0.31)
   - Chunk D (Score: 0.05) - sÃ³ se tiver menos de 4 chunks melhores
```

#### âœ… Resumo: Como Funciona

1. **Embeddings** transformam texto em vetores que capturam **significado**
2. **Qdrant** compara vetores usando **distÃ¢ncia cosseno**
3. Retorna os chunks mais **similares** (nÃ£o por palavras, mas por **significado**)
4. O modelo recebe apenas os **textos** dos chunks mais relevantes

#### ğŸ†š Por Que NÃ£o Busca por Palavras-Chave?

**Busca por palavras-chave:**
```
Pergunta: "qual a profissÃ£o?"
Busca: encontra apenas chunks com a palavra "profissÃ£o"
Problema: perde "trabalho", "cargo", "atuaÃ§Ã£o"
```

**Busca semÃ¢ntica (vetorial):**
```
Pergunta: "qual a profissÃ£o?"
Busca: encontra chunks com significado similar
Resultado: encontra "Engenheiro", "Desenvolvedor", "Programador"
```

**Vantagem:** Funciona mesmo com palavras diferentes que tÃªm o mesmo significado!

---

### 2.7. Resumo das DecisÃµes TÃ©cnicas

| Item | DÃºvida/Contexto | Onde estÃ¡ documentado | SoluÃ§Ã£o Adotada |
|------|----------------|----------------------|-----------------|
| **Redis (Inicialmente)** | "Devo usar Redis para o histÃ³rico? Posso usar variÃ¡vel global em Python/Node? O Qdrant salva o histÃ³rico? Banco nÃ£o relacional / relacional funciona aqui?" | **SeÃ§Ã£o 2.1** | Descartado por custo/complexidade (VPC). Usamos LocalStorage no frontend. |
| **IdentificaÃ§Ã£o UsuÃ¡rio** | Como identificar usuÃ¡rios sem autenticaÃ§Ã£o? | **SeÃ§Ã£o 2.1** | UUID no LocalStorage para logs/analytics. Enviado via header `x-user-id`. |
| **Banco Vetorial vs. Relacional** | "Por que um banco vetorial? Por que nÃ£o extrair o JSON do texto e jogar no prompt?" | **SeÃ§Ã£o 2.4** | Qdrant resolve o problema de Context Window (limite de tokens). Busca semÃ¢ntica. |
| **WebSocket vs. Lambda** | Como fazer conexÃ£o cliente-servidor sabendo que Lambda sobe e morre? WebSockets ou HTTP resolve? | **SeÃ§Ã£o 2.2** | WebSocket complexo demais. Usamos HTTP Streaming com `RESPONSE_STREAM`. |
| **LangChain vs. Na mÃ£o** | "Vamos usar LangChain ou chamar a OpenAI direto? LangChain nÃ£o Ã© complexo para streaming?" | **SeÃ§Ã£o 2.5** | Sim. Nova sintaxe (`.stream()`) simplificou. Processa PDF automaticamente. |
| **MemÃ³ria Persistente** | Onde salvar o histÃ³rico? | **SeÃ§Ã£o 2.1** | Frontend Ã© o "dono" do estado. Envia histÃ³rico completo a cada request. |
| **Config AWS Lambda (Streaming)** | Como ativar streaming na Lambda? | **SeÃ§Ã£o 5.1** | `invokeMode: RESPONSE_STREAM` no `serverless.yml`. |
| **Nuxt Nitro vs. Express** | Por que Nitro e nÃ£o Express? | **SeÃ§Ã£o 2.3** | Nitro Ã© nativo do Nuxt, evita cold start, monorepo simplificado. |
| **Function URL vs. API Gateway** | Qual usar para evitar buffer? | **SeÃ§Ã£o 1** | Function URL direto, sem passar pelo API Gateway tradicional. |
| **PDF Processing** | pdf-parse ou LangChain? | **Package.json** | 100% LangChain com `PDFLoader`. Mais integrado e sem deps extras. |
| **Upload de PDF** | Script local ou interface web? | **SeÃ§Ã£o 5.1.1** | Interface web com drag & drop. Endpoint `/api/ingest` processa upload. |

---

### 2.8. Arquitetura Serverless: Nuxt Full-Stack vs. Node.js + Vue Separados

#### ğŸ¯ A DecisÃ£o: Por que Nuxt MonolÃ­tico para MVP?

Este projeto usa **Nuxt Full-Stack** (Frontend + Backend no mesmo Lambda). Mas por quÃª? E como seria com separaÃ§Ã£o tradicional?

#### ğŸ“¦ Nuxt Full-Stack (Atual)

**Estrutura:**
```
Seu Projeto
â”œâ”€â”€ pages/          â† Frontend (Vue components renderizados)
â”œâ”€â”€ server/         â† Backend (Node.js/H3 endpoints)
â””â”€â”€ nuxt.config.ts  â† ConfiguraÃ§Ã£o Ãºnica
```

**Build & Deploy:**
```
pnpm run build
         â†“
    .output/
    â”œâ”€â”€ public/      â† Assets estÃ¡ticos (JS, CSS)
    â”œâ”€â”€ server/      â† Node.js compilado
    â””â”€â”€ index.mjs    â† Entry point Ãºnico
         â†“
    Upload para AWS Lambda (Ãºnico pacote)
```

**Em ExecuÃ§Ã£o (Lambda):**
```
Usuario acessa: https://api.lambda.amazonaws.com/
                         â†“
                    Lambda Function
                    (index.mjs inicia)
                         â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“                             â†“
    GET /            GET /api/chat
  (Renderiza HTML)  (LangChain + RAG)
   (Frontend Vue)    (Backend Node.js)
```

**Vantagens para MVP:**
- âœ… **CÃ³digo compartilhado**: Types, utilities, validaÃ§Ãµes entre frontend e backend
- âœ… **Uma Ãºnica build**: Simplifica CI/CD
- âœ… **Deploy simples**: Um arquivo ZIP para Lambda
- âœ… **SSR/Rendering**: Nuxt renderiza HTML no servidor (melhora SEO, primeira carga mais rÃ¡pida)
- âœ… **Monorepo implÃ­cito**: Sem necessidade de gerenciar mÃºltiplos repos

**Desvantagens:**
- âŒ **Cold starts maiores**: Bundle = LangChain (~5MB) + Vue (~500KB) + Qdrant (~10MB) = 15-30MB zipped
  - Resultado: 10-30 segundos no primeiro request â±ï¸
- âŒ **Limite de tamanho**: Lambda tem limite de 50MB (zipped), vocÃª fica perto do limite
- âŒ **Scaling desacoplado impossÃ­vel**: Frontend e backend escalam juntos (nÃ£o ideal em produÃ§Ã£o)

---

#### ğŸ—ï¸ Node.js + Vue Separados (ProduÃ§Ã£o)

**Arquitetura Desacoplada:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 + CDN      â”‚         â”‚   Lambda     â”‚
â”‚  (Vue Build)    â”‚         â”‚  (Node API)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
    estÃ¡ticos.com          api.estÃ¡ticos.com
      (S3+CF)               (API puro)
```

**Build Diferenciado:**
```bash
# Frontend: Build estÃ¡tico
pnpm run build:frontend
    â†“
dist/
â”œâ”€â”€ index.html
â”œâ”€â”€ js/
â”œâ”€â”€ css/
â””â”€â”€ assets/
    â†“
Upload para S3 â†’ CloudFront (CDN)

# Backend: API isolada
pnpm run build:api
    â†“
.output-api/
â”œâ”€â”€ server/
â””â”€â”€ index.mjs
    â†“
Upload para Lambda
```

**Em ExecuÃ§Ã£o:**
```
Usuario acessa: https://estÃ¡ticos.com
                      â†“
    CloudFront serve HTML/JS/CSS
    (Assets estÃ¡ticos prÃ©-compilados)
                      â†“
   Script Vue executa no navegador
                      â†“
   POST /api/chat â†’ https://api.estÃ¡ticos.com
                      â†“
              Lambda Function
          (Node.js + LangChain puro)
                      â†“
         Retorna resposta com streaming
```

**Vantagens para ProduÃ§Ã£o:**
- âœ… **Cold starts reduzidos**: Lambda contÃ©m apenas LangChain (~15MB) + dependÃªncias
  - Resultado: 3-5 segundos no primeiro request
- âœ… **Scaling independente**: Se frontend Ã© acessado muito, CDN absorve. Se backend Ã© pesado, Lambda escala Ã  parte
- âœ… **Cache agressivo**: S3 + CloudFront cacheia assets por anos
- âœ… **Assets globais**: CDN distribui JS/CSS pelo mundo
- âœ… **SeparaÃ§Ã£o de responsabilidades**: Frontend versionado separado do backend

**Desvantagens:**
- âŒ **Complexidade aumenta**: Gerenciar 2 builds, 2 deployments, 2 repos
- âŒ **Setup inicial**: Precisa configurar CloudFront, S3, origem do CORS
- âŒ **Custo**: S3 + CloudFront adiciona custo (porÃ©m mÃ­nimo para MVP)

---

#### ğŸ“Š ComparaÃ§Ã£o Lado a Lado

| Aspecto | Nuxt Full-Stack (Atual) | Node + Vue Separados |
|--------|-------------------------|----------------------|
| **Cold Start** | 10-30s | 3-5s |
| **Tamanho Bundle** | 20-30MB | 15MB (API) + 2MB (Frontend) |
| **Complexidade** | Baixa | MÃ©dia |
| **Deploy** | Um comando | Dois comandos (2 pipelines) |
| **Cache Frontend** | Nenhum (sempre do Lambda) | Agressivo (CDN) |
| **Custo Mensal** | ~$0.20 (poucos reqs) | ~$1-3 (inclui CDN) |
| **Escalabilidade** | Ruim | Excelente |
| **Recomendado para** | MVP, protÃ³tipos | ProduÃ§Ã£o, alto trÃ¡fego |

---

#### ğŸ¬ Roadmap Sugerido

1. **Fase 1 (MVP - Atual)**: Nuxt Full-Stack
   - VÃ¡lido para <100 reqs/dia
   - Prototipa rÃ¡pido
   - Deploy simples

2. **Fase 2 (Beta - ~1 mÃªs)**: SeparaÃ§Ã£o de builds
   - MantÃ©m monorepo Ãºnico
   - Scripts `build:frontend` e `build:api`
   - Deployments independentes mas no mesmo repo

3. **Fase 3 (ProduÃ§Ã£o)**: Monorepo split
   - 2 repositÃ³rios (`frontend` e `backend-api`)
   - CI/CD pipelines completamente separadas
   - Scaling independente

---

#### ğŸ’¡ Resposta TÃ©cnica: Como Funciona?

**Por que ambos vÃ£o juntos para Lambda atualmente?**

Quando vocÃª executa `pnpm run build` no Nuxt:

1. Nuxt compila o Vue em componentes reativos
2. Nuxt compila o `/server` em mÃ³dulos Node.js
3. Tudo Ã© empacotado em `.output/` com um Ãºnico `index.mjs`
4. VocÃª faz zip de `.output/` e upload para Lambda
5. Lambda executa `index.mjs`, que:
   - Inicia um servidor Node.js na porta 3000
   - Quando recebe GET `/`, renderiza HTML do Vue
   - Quando recebe POST `/api/chat`, executa o handler do server

**Resultado**: Uma Ãºnica funÃ§Ã£o Lambda atende ambos.

**Vantagem futura**: VocÃª pode exportar apenas o `/server` compilado para uma Lambda separada sem alterar cÃ³digo (apenas CI/CD).


## 3. Fluxo de Trabalho Git (PadrÃ£o Standard)

Seguiremos rigorosamente este fluxo para organizaÃ§Ã£o.

### Regras de Branch
1. **Nunca** commitar na `main` ou `develop` diretamente
2. Toda branch nasce da `develop`
3. Toda branch morre (merge) na `develop`

### Nomenclatura
PadrÃ£o: `DDMMYY-Feature-Description`

#### Formato
- **DDMMYY**: Data no formato dia/mÃªs/ano (2 dÃ­gitos cada)
- **Feature**: Nome da feature em kebab-case
- **Description**: DescriÃ§Ã£o adicional em kebab-case

#### Exemplos
041125-DB-schemas-review-refactor
031225-Notifications-V2-implementation
031225-Alerts-refactor-remove-airlines
031225-Redis-optimization-monthly-quotas
031225-Moblix-V2-integration
#### Regras
- Use kebab-case (palavras separadas por hÃ­fens)
- Seja descritivo mas conciso
- Inclua a data de criaÃ§Ã£o da branch
- Use maiÃºsculas apenas para siglas (V2, API, DB)

### Fluxo Simplificado

#### Estrutura
```
Feature Branch â†’ develop â†’ master
```

#### Fluxo Detalhado

1. **Criar branch de feature**
   ```bash
   git checkout develop
   git pull origin develop
   git checkout -b 081225-project-setup
   ```

2. **Desenvolver feature**
   - MÃºltiplos commits organizados
   - Commits seguindo Conventional Commits
   - Data jÃ¡ estÃ¡ no nome da branch (nÃ£o repetir nos commits)

3. **Criar Pull Request**
   - PR: `081225-project-setup` â†’ `develop`
   - Adicionar descriÃ§Ã£o clara
   - Linkar issues relacionadas

4. **Merge em develop**
   - ApÃ³s code review e aprovaÃ§Ã£o
   - Testes passando
   - Sem conflitos
   - Merges frequente na branch develop

5. **Merge em master**
   - Apenas quando funÃ§Ã£o de alto nÃ­vel estiver completa
   - Testes passando
   - DocumentaÃ§Ã£o atualizada
   - CoordenaÃ§Ã£o com frontend alinhada

### Regra: Sempre Partir de Develop

**Todas as branches de feature devem SEMPRE ser criadas a partir de `develop`.**

#### Por quÃª?

1. **`develop` Ã© a branch de integraÃ§Ã£o estÃ¡vel**
   - ContÃ©m todas as features jÃ¡ mergeadas e testadas
   - Ã‰ o ponto de referÃªncia comum para todo o time

2. **Evita dependÃªncias entre features**
   - Cada feature Ã© independente
   - NÃ£o cria dependÃªncias desnecessÃ¡rias entre branches

3. **Facilita o merge posterior**
   - Menos conflitos ao fazer merge em `develop`
   - HistÃ³rico mais limpo e organizado

4. **MantÃ©m todas as features no mesmo ponto de partida**
   - Garante consistÃªncia entre diferentes features
   - Facilita code review e testes

#### Workflow Correto

```bash
# 1. SEMPRE voltar para develop primeiro
git checkout develop

# 2. Atualizar develop com o remoto (importante!)
git pull origin develop

# 3. Criar nova branch de feature a partir de develop atualizada
git checkout -b 081225-project-setup

# 4. Agora trabalhar na feature
git add <arquivos>
git commit -m "feat(setup): ..."
```

#### O Que NÃƒO Fazer

```bash
# âŒ ERRADO - Criar branch a partir de outra feature branch
git checkout 091225-backend-logic
git checkout -b 101225-frontend-ui  # ERRADO!

# âŒ ERRADO - Criar branch sem atualizar develop primeiro
git checkout develop
git checkout -b 081225-project-setup  # Pode estar desatualizada!
```

### PrincÃ­pios
- **Features**: Merge direto em `develop` via PR
- **Master**: Merge apenas quando funÃ§Ã£o completa e testada
- **Develop**: Sempre estÃ¡vel para basear trabalho

### Conventional Commits

#### Formato
```
<tipo>(<escopo>): <descriÃ§Ã£o>
```

#### Tipos Comuns
- `feat`: Nova funcionalidade
- `fix`: CorreÃ§Ã£o de bug
- `docs`: DocumentaÃ§Ã£o
- `style`: FormataÃ§Ã£o (sem mudanÃ§a de cÃ³digo)
- `refactor`: RefatoraÃ§Ã£o
- `perf`: Melhoria de performance
- `test`: Testes
- `chore`: Tarefas de manutenÃ§Ã£o
- `ci`: CI/CD
- `build`: Build system

#### Exemplos
```bash
feat(auth): adiciona login com Google OAuth
fix(payment): corrige cÃ¡lculo de desconto
docs(readme): atualiza instruÃ§Ãµes de instalaÃ§Ã£o
refactor(api): simplifica validaÃ§Ã£o de dados
```

#### Primeira Linha
- Use o imperativo: "adiciona", "corrige", "atualiza"
- NÃ£o use: "adicionado", "corrigido", "atualizado"
- Seja especÃ­fico e claro
- Evite mensagens genÃ©ricas como "update" ou "fix"

#### Boas PrÃ¡ticas
- Um commit = uma mudanÃ§a lÃ³gica
- Commits pequenos e frequentes
- Teste antes de commitar
- Use o corpo para explicar o "porquÃª" quando necessÃ¡rio

#### Estrutura Completa

```
<tipo>(<escopo>): <assunto curto>

<corpo opcional explicando o porquÃª>

<rodapÃ© opcional com referÃªncias>
```

#### Exemplo Completo
```bash
feat(api): adiciona endpoint de busca de usuÃ¡rios

Implementa busca paginada com filtros por nome e email.
Isso melhora a performance em listas grandes de usuÃ¡rios.

Closes #123
```

#### Exemplos de Boas Mensagens
```bash
feat(chat): adiciona componente ChatMessage
feat(api): implementa endpoint de streaming
feat(rag): integra Qdrant para busca vetorial
refactor(ui): simplifica lÃ³gica de LocalStorage
fix(stream): corrige encoding de caracteres especiais
docs(readme): atualiza instruÃ§Ãµes de instalaÃ§Ã£o
```

#### Exemplos de Mensagens a Evitar
```bash
# âŒ Evite
update
fix
changes
WIP
test

# âœ… Prefira
feat(chat): adiciona componente ChatMessage
fix(stream): corrige encoding de caracteres especiais
refactor(ui): simplifica lÃ³gica de LocalStorage
```

---

## 4. Roteiro de ImplementaÃ§Ã£o (Roadmap)

Siga esta ordem exata para evitar bloqueios.

### Fase 0: Setup Inicial
* **Branch:** `project-setup`
* **Tarefas:**
    * Init Nuxt 3
    * Install LangChain/OpenAI libs
    * Configurar `.env`

### Fase 1: O "CÃ©rebro" (IngestÃ£o de Dados)
* **Branch:** `rag-ingestion-implementation`
* **Tarefas:**
    * Criar endpoint `server/api/ingest.post.ts`
    * Receber PDF via multipart/form-data
    * Processar com LangChain (`PDFLoader` + `RecursiveCharacterTextSplitter`)
    * Gerar Embeddings e salvar no Qdrant
    * Criar componente `DocumentUpload.vue` (drag & drop)
* *Nota:* O upload agora Ã© feito via interface web, nÃ£o por script local

### Fase 2: O Backend LÃ³gico
* **Branch:** `backend-logic-implementation`
* **Tarefas:**
    * Criar rota Nitro `server/api/chat.post.ts`
    * Receber `{ question, history }`
    * Configurar conexÃ£o Qdrant + OpenAI via LangChain
    * Testar resposta texto simples (sem stream)

### Fase 3: O Frontend Visual
* **Branch:** `frontend-ui-implementation`
* **Tarefas:**
    * Layout do Chat
    * LÃ³gica de `LocalStorage` (Salvar histÃ³rico)
    * Envio do Payload completo para a API

### Fase 4: O Streaming (IntegraÃ§Ã£o Final)
* **Branch:** `streaming-implementation`
* **Tarefas:**
    * **Back:** Mudar resposta para `sendStream` com iterÃ¡vel do LangChain
    * **Front:** Mudar `fetch` para usar `getReader()` e montar texto em tempo real
    * **Infra:** Configurar `serverless.yml` com `invokeMode: RESPONSE_STREAM`

---

## 5. ImplementaÃ§Ã£o TÃ©cnica de ReferÃªncia

### 5.1 Endpoints da API

O projeto possui **2 endpoints principais**:

#### 5.1.1 `/api/ingest` - Upload de Documentos

Endpoint para fazer upload de PDFs e processar para o Qdrant.

**Request:**
```http
POST /api/ingest
Content-Type: multipart/form-data

file: <PDF_FILE>
```

**Response:**
Retorna JSON com status de sucesso, nÃºmero de chunks processados e ID do documento.

**Detalhes de ImplementaÃ§Ã£o:**
Ver cÃ³digo em `server/api/ingest.post.ts` para implementaÃ§Ã£o completa com PDFLoader, RecursiveCharacterTextSplitter e integraÃ§Ã£o Qdrant.

#### 5.1.2 `/api/chat` - ConversaÃ§Ã£o com Streaming

Endpoint para conversar com o chatbot. Retorna resposta via streaming.

**Request:**
```http
POST /api/chat
Content-Type: application/json
x-user-id: <UUID>

{
  "question": "Qual Ã© o conteÃºdo do documento?",
  "history": [
    { "role": "user", "content": "Pergunta anterior" },
    { "role": "assistant", "content": "Resposta anterior" }
  ]
}
```

**Response:**
```http
Content-Type: text/event-stream

[Streaming de texto em tempo real]
```

**ImplementaÃ§Ã£o:** Ver seÃ§Ã£o 5.2

### 5.2 Arquivo de ConfiguraÃ§Ã£o (`serverless.yml`)

ConfiguraÃ§Ã£o vital para o streaming funcionar na AWS.

**Pontos Principais:**
- `invokeMode: RESPONSE_STREAM` - Essencial para streaming funcionar
- `timeout: 30` - Timeout de 30 segundos para processar requisiÃ§Ãµes
- `memorySize: 512` - MemÃ³ria RAM alocada para a Lambda
- VariÃ¡veis de ambiente carregadas via `serverless-dotenv-plugin`

### 5.3 LÃ³gica de Streaming no Nitro (`server/api/chat.post.ts`)

**Fluxo de Streaming:**
1. Recebe `{ question, history }` do frontend
2. Transforma histÃ³rico JSON em objetos LangChain (HumanMessage, AIMessage)
3. Configura ChatOpenAI com `streaming: true`
4. Cria Chain com prompt template + LLM + output parser
5. Configura headers HTTP (`Content-Type: text/event-stream`)
6. Executa chain e converte para ReadableStream
7. Retorna stream usando `sendStream(event, readable)`

**Ver implementaÃ§Ã£o completa em:** `server/api/chat.post.ts`

### 5.4 Guia de ImplementaÃ§Ã£o AWS Serverless (Credenciais + Deploy)

**Resumo direto**
- **Para que servem as chaves (Access Key / Secret):** credenciais de usuÃ¡rio IAM para o CLI (Serverless Framework / AWS CLI) autenticar na sua conta AWS e criar/atualizar recursos. Sem elas, o `serverless deploy` nÃ£o tem permissÃ£o para subir nada.
- **Quem usa:** o prÃ³prio `serverless deploy` (via AWS SDK) e qualquer comando AWS CLI. NÃ£o sÃ£o usadas pelo cÃ³digo do app em runtime e nÃ£o vÃ£o dentro do bundle.
- **Onde ficam:** o comando `serverless config credentials --provider aws --key ... --secret ...` salva em `~/.aws/credentials` (e `~/.aws/config`), fora do projeto e fora do Git.
- **RelaÃ§Ã£o com `serverless.yml`:**
  - O `serverless.yml` descreve o que criar na AWS: serviÃ§o `rag-chatbot-mvp`, runtime `nodejs18.x`, regiÃ£o `us-east-1`, funÃ§Ã£o `api` apontando para `.output/server/index.handler`, URL pÃºblica com CORS e streaming, timeout/memÃ³ria e variÃ¡veis de ambiente do Lambda.
  - Ao rodar `serverless deploy`, o Serverless Framework lÃª o `serverless.yml`, empacota o handler e usa as credenciais do `~/.aws/credentials` para provisionar/atualizar a Lambda + URL.
  - As variÃ¡veis em `environment:` (`OPENAI_API_KEY`, `QDRANT_URL`, etc.) sÃ£o injetadas no runtime da Lambda a partir do ambiente do deploy (via `.env` + `serverless-dotenv-plugin` ou variÃ¡veis do CI). **NÃ£o** devem conter as chaves IAM; essas ficam sÃ³ no perfil local/CI para autenticaÃ§Ã£o do deploy.

**Resumo prÃ¡tico**
1) Guarde as chaves IAM apenas em `~/.aws/credentials` (ou variÃ¡veis de ambiente no CI).
2) Coloque apenas as variÃ¡veis do app em `.env` (OpenAI/Qdrant etc.).
3) Rode `pnpm build` para gerar `.output/server/index.handler`.
4) Rode `serverless deploy`: ele usa as credenciais locais para falar com a AWS e criar/atualizar a Lambda conforme o `serverless.yml`.

#### ğŸ“Œ Guia passo a passo (AWS + Serverless)

##### Fase 1: O Terreno (AWS Root)
Objetivo: ter uma conta ativa na nuvem.
1. Acessar `aws.amazon.com` e criar a conta (root user) com email, pagamento e verificaÃ§Ã£o.

##### Fase 2: O CrachÃ¡ do RobÃ´ (IAM User)
Objetivo: gerar chaves para o Serverless Framework atuar na conta.
1. No console AWS, abrir **IAM > Users > Create User** (ex: `serverless-admin`).
2. Em **Permissions**, usar **Attach policies directly** â†’ `AdministratorAccess` (para facilitar o MVP).
3. Abrir o usuÃ¡rio criado â†’ aba **Security Credentials** â†’ **Access Keys** â†’ **Create access key** â†’ opÃ§Ã£o **Command Line Interface (CLI)**.
4. Copiar `Access Key ID` e `Secret Access Key` (guardar em local seguro).

##### Fase 3: Configurar o "crachÃ¡" no Serverless Framework
Objetivo: instalar a ferramenta e armazenar as credenciais localmente.
```bash
pnpm add -g serverless
serverless config credentials --provider aws --key SUA_ACCESS_KEY --secret SUA_SECRET_KEY
```
*Esse comando salva em `~/.aws/credentials` para o Serverless usar sempre. NÃ£o vai para o repo.*

##### Fase 4: Preparar o Projeto Nuxt
Objetivo: deixar o cÃ³digo pronto para empacotar.
```bash
pnpm add -D serverless-dotenv-plugin

# Criar .env na raiz (exemplo)
OPENAI_API_KEY=sk-proj-...
QDRANT_URL=https://...
QDRANT_API_KEY=th-...

# Conferir serverless.yml (resumo)
service: rag-chatbot-mvp
frameworkVersion: '3'
provider:
  name: aws
  runtime: nodejs18.x
  region: us-east-1
  environment:
    OPENAI_API_KEY: ${env:OPENAI_API_KEY}
    QDRANT_URL: ${env:QDRANT_URL}
    QDRANT_API_KEY: ${env:QDRANT_API_KEY}
functions:
  api:
    handler: .output/server/index.handler
    url:
      cors: true
      invokeMode: RESPONSE_STREAM
    timeout: 30
plugins:
  - serverless-dotenv-plugin
```

##### Fase 5: Build e Deploy
Objetivo: compilar o Nuxt e enviar para a AWS.
```bash
pnpm build          # gera .output
serverless deploy   # lÃª o serverless.yml e faz o upload
```
ApÃ³s o deploy, o terminal retorna a Function URL pÃºblica (ex: `https://xyz.lambda-url.us-east-1.on.aws`).

---

### ğŸ“¦ O que `pnpm run deploy:api` faz?

O script `deploy:api` executa dois comandos em sequÃªncia:

```bash
"deploy:api": "pnpm build && serverless deploy --verbose"
```

#### 1ï¸âƒ£ `pnpm build` â†’ Executa `nuxt build`:
- Compila o cÃ³digo Vue/Nuxt (TypeScript â†’ JavaScript)
- Gera a pasta `.output/server/` com o cÃ³digo otimizado para Lambda
- Faz **tree-shaking** (remove cÃ³digo nÃ£o usado)
- Faz **minificaÃ§Ã£o** (reduz tamanho dos arquivos)
- Faz **bundling** de dependÃªncias em um Ãºnico arquivo (`index.mjs` de ~208 KB)

#### 2ï¸âƒ£ `serverless deploy` â†’ Pega o `.output/server/` e:
- Cria o ZIP (~517 KB)
- Faz upload para S3
- Cria/atualiza a Lambda Function na AWS
- Retorna a Function URL pÃºblica

#### ğŸ¤” Por que o build Ã© necessÃ¡rio?

| Uso | Precisa do Build? | Motivo |
|-----|-------------------|--------|
| **Lambda (API)** | âœ… SIM | Lambda precisa do cÃ³digo compilado em `.output/server/` |
| **Frontend Docker Local** | âœ… SIM | TambÃ©m usa o build do Nuxt, mas com preset diferente (`node-server`) |

O build Ã© **essencial** para o Lambda porque:

1. **Transforma TypeScript â†’ JavaScript**: Lambda nÃ£o executa TypeScript nativamente
2. **Bundla dependÃªncias**: Todas as deps sÃ£o empacotadas em um Ãºnico arquivo (`index.mjs`)
3. **Otimiza para produÃ§Ã£o**: Remove cÃ³digo de desenvolvimento, minifica, tree-shake
4. **Reduz tamanho**: De ~500 MB (`node_modules` raiz) para ~2 MB (`.output/server/`)

**Resultado final do build:**
```
.output/server/
â”œâ”€â”€ index.mjs       (208 KB) â† CÃ³digo da aplicaÃ§Ã£o + deps bundladas
â”œâ”€â”€ package.json    (673 B)  â† Metadados mÃ­nimos
â””â”€â”€ node_modules/   (~1.5 MB) â† Apenas deps que nÃ£o podem ser bundladas
```

---

## 6. ğŸ“¦ OtimizaÃ§Ã£o de Bundle: API-Only vs Full-Stack

### ğŸ¯ Por que Full-Stack funcionou mas API-Only nÃ£o?

Durante o desenvolvimento, encontramos um problema curioso: o modo **Full-Stack funcionou de primeira**, mas o **API-Only dava erro de tamanho** (130 MB). 

**A resposta:** O problema **nunca foi o build do Nuxt** - foi a **configuraÃ§Ã£o do `serverless.yml`!**

---

#### ğŸ” O que realmente aconteceu

O build do Nuxt **sempre funcionou corretamente** em ambos os modos:

```
Build do Nuxt (.output/):
â”œâ”€â”€ public/     (500 KB)  â† Assets frontend (sÃ³ no Full-Stack)
â””â”€â”€ server/     (2 MB)    â† Backend otimizado
    â””â”€â”€ node_modules/ (1.5 MB) â† Deps bundladas
```

O problema estava no **empacotamento do Serverless Framework**, nÃ£o no build.

---

#### âœ… Full-Stack (funcionou de primeira)

```yaml
# serverless.yml Full-Stack
package:
  patterns:
    - '.output/**'           # Inclui .output/public + .output/server
    - '!node_modules/**'     # Exclui node_modules raiz
```

**O que o Serverless empacotava:**
```
ZIP enviado (~3.5 MB):
â”œâ”€â”€ .output/public/  âœ… (500 KB)
â”œâ”€â”€ .output/server/  âœ… (2 MB)
â””â”€â”€ node_modules/    âŒ ExcluÃ­do corretamente
```

**Por que funcionou?** O pattern `.output/**` era especÃ­fico o suficiente para que o Serverless nÃ£o "vazasse" outros arquivos.

---

#### âŒ API-Only (nÃ£o funcionou - ANTES de arrumar)

```yaml
# serverless.yml API-Only (ANTES)
package:
  patterns:
    - '.output/server/**'    # Inclui server
    - '!node_modules/**'     # DEVERIA excluir, mas...
```

**O que o Serverless REALMENTE empacotava:**
```
ZIP enviado (~130 MB):
â”œâ”€â”€ .output/server/  âœ… (2 MB)
â”œâ”€â”€ node_modules/    âŒ (500 MB) â† DA RAIZ! ğŸ˜±
```

**Por que falhou?** O Serverless Framework faz um **glob match** na raiz do projeto. Como `.output/server/**` nÃ£o cobre "tudo", ele ainda procurava outros arquivos e **encontrava o `node_modules/` da raiz do projeto**.

A exclusÃ£o `!node_modules/**` nÃ£o funcionava bem porque a **ordem dos patterns** estava errada.

---

#### âœ… API-Only (funcionou - DEPOIS de arrumar)

```yaml
# serverless.yml API-Only (DEPOIS)
package:
  patterns:
    - '!**'                          # 1ï¸âƒ£ Exclui ABSOLUTAMENTE TUDO
    - '!node_modules/**'             # 2ï¸âƒ£ Garante exclusÃ£o extra
    - '.output/server/index.mjs'     # 3ï¸âƒ£ Inclui APENAS o necessÃ¡rio
    - '.output/server/package.json'
    - '.output/server/node_modules/**'
```

**O que o Serverless empacota agora:**
```
ZIP enviado (~517 KB):
â”œâ”€â”€ .output/server/index.mjs      âœ… (208 KB)
â”œâ”€â”€ .output/server/package.json   âœ… (673 B)
â””â”€â”€ .output/server/node_modules/  âœ… (1.5 MB)
```

**Por que funciona?** O `!**` exclui **absolutamente tudo** primeiro, e depois incluÃ­mos **apenas** os arquivos especÃ­ficos que precisamos.

---

#### ğŸ“Š Tabela Comparativa Final

| Aspecto | Full-Stack | API-Only (antes) | API-Only (depois) |
|---------|------------|------------------|-------------------|
| **Build Nuxt** | âœ… Mesmo (~2 MB) | âœ… Mesmo (~2 MB) | âœ… Mesmo (~2 MB) |
| **O que QUERIA enviar** | `.output/` inteiro | Apenas `.output/server/` | Apenas `.output/server/` |
| **O que REALMENTE enviou** | âœ… `.output/` (~3.5 MB) | âŒ `.output/server/` + `node_modules/` raiz (~130 MB) | âœ… `.output/server/` (~517 KB) |
| **Pattern inicial** | `.output/**` | `.output/server/**` | `!**` |
| **node_modules raiz incluÃ­do?** | âŒ NÃ£o | âœ… SIM (bug!) | âŒ NÃ£o |
| **Tamanho ZIP final** | ~3.5 MB âœ… | ~130 MB âŒ | ~517 KB âœ… |

---

#### ğŸ’¡ LiÃ§Ã£o Aprendida

**Resumo:** Nos dois casos (Full-Stack e API-Only):
1. VocÃª fazia o build â†’ `pnpm build` â†’ gerava `.output/` otimizado (~2-3 MB)
2. VocÃª jogava a build no Lambda â†’ `serverless deploy`

**O que aconteceu:**
- âœ… O build **sempre funcionou**
- âœ… A lÃ³gica de "Full-Stack = tudo, API-Only = sÃ³ server" estava **certa**
- âŒ O `serverless.yml` do API-Only tinha patterns que **vazavam o `node_modules/` raiz**

**Por que Full-Stack funcionou "por sorte":**  
O pattern `.output/**` era especÃ­fico o suficiente para que o Serverless nÃ£o "vazasse" outros arquivos.

**Por que API-Only falhou:**  
O pattern `.output/server/**` era menos abrangente, e o Serverless Framework ainda buscava outros arquivos na raiz, encontrando o `node_modules/` de 500 MB.

> **A soluÃ§Ã£o:** `'!**'` primeiro para garantir que **nada vaze**.

---

### ğŸŒ httpApi vs Function URL: Por que um funcionou e o outro nÃ£o?

Durante o desenvolvimento, testamos duas abordagens para expor a Lambda:

#### âŒ Function URL (nÃ£o funcionou inicialmente)

```yaml
functions:
  api:
    url:
      cors: true
      invokeMode: RESPONSE_STREAM
```

**Problema:** O Serverless Framework v3 criava a Function URL com `AuthType: AWS_IAM` por padrÃ£o, exigindo credenciais AWS assinadas para acesso. Resultado: **403 Forbidden** para requisiÃ§Ãµes pÃºblicas.

**Por que isso aconteceu:**
- Function URLs sÃ£o um recurso mais novo da AWS (2022)
- O Serverless Framework nÃ£o tinha controle total sobre o `AuthType` via sintaxe simples
- Mesmo especificando `cors: true`, a URL era criada como privada

**SoluÃ§Ã£o temporÃ¡ria:** Executar `aws lambda update-function-url-config --auth-type NONE` manualmente apÃ³s cada deploy.

**SoluÃ§Ã£o permanente:** Adicionar recurso CloudFormation customizado para forÃ§ar `AuthType: NONE`.

---

#### âœ… API Gateway HTTP API (funcionou de primeira)

```yaml
functions:
  api:
    events:
      - httpApi: '*'
```

**Por que funcionou:**
- API Gateway HTTP API Ã© um serviÃ§o mais maduro e amplamente suportado
- O Serverless Framework tem controle total sobre as configuraÃ§Ãµes
- Por padrÃ£o, cria endpoints **pÃºblicos** (sem autenticaÃ§Ã£o)
- Rota coringa `*` captura todas as requisiÃ§Ãµes e repassa para a Lambda

**Vantagens:**
- âœ… Deploy determinÃ­stico (sempre pÃºblico)
- âœ… CORS configurÃ¡vel via provider
- âœ… Suporte a custom domains
- âœ… MÃ©tricas e logs integrados

**Desvantagens:**
- âŒ Limite de timeout de 30 segundos (vs 15 minutos na Function URL)
- âŒ NÃ£o suporta `RESPONSE_STREAM` nativo (streaming funciona mas com overhead)
- âŒ Custo adicional mÃ­nimo (mas irrelevante para MVP)

---

#### ğŸ“Š ComparaÃ§Ã£o Final

| Aspecto | Function URL | API Gateway HTTP API |
|---------|--------------|----------------------|
| **AuthType padrÃ£o (Serverless)** | `AWS_IAM` âŒ | PÃºblico âœ… |
| **Timeout mÃ¡ximo** | 15 minutos | 30 segundos |
| **Streaming nativo** | âœ… `RESPONSE_STREAM` | âš ï¸ Via chunks |
| **ConfiguraÃ§Ã£o** | Requer CloudFormation extra | âœ… Funciona out-of-the-box |
| **Custo** | Gratuito | +$1/milhÃ£o de requisiÃ§Ãµes |
| **RecomendaÃ§Ã£o MVP** | âš ï¸ Requer setup extra | âœ… **Use este** |

---

#### ğŸ¤” ConfusÃ£o Comum: "Mas httpApi nÃ£o aceita streaming?"

**Resposta curta:** Aceita sim! Mas de forma diferente.

**Como funciona:**

| MÃ©todo | Function URL (`RESPONSE_STREAM`) | API Gateway (`httpApi`) |
|--------|----------------------------------|-------------------------|
| **Tipo** | Streaming nativo da Lambda | HTTP Transfer-Encoding: chunked |
| **ImplementaÃ§Ã£o** | Lambda envia chunks diretamente | Lambda retorna body, Gateway repassa em chunks |
| **Cliente** | Recebe chunks em tempo real | Recebe chunks em tempo real |
| **Resultado** | âœ… Funciona | âœ… Funciona |

**Em ambos os casos**, seu cÃ³digo no cliente usa o **mesmo** `fetch` + `ReadableStream`:

```ts
const response = await fetch('/api/chat', { method: 'POST', body: ... })
const reader = response.body.getReader()
while (true) {
  const { done, value } = await reader.read()
  if (done) break
  console.log(new TextDecoder().decode(value)) // chunks chegam aqui
}
```

**A diferenÃ§a estÃ¡ no backend:**

```ts
// Function URL (RESPONSE_STREAM)
export const handler = awslambda.streamifyResponse(async (event, responseStream) => {
  responseStream.write('chunk1\n')
  responseStream.write('chunk2\n')
  responseStream.end()
})

// API Gateway (HTTP chunked) - mais simples!
export default defineEventHandler(async (event) => {
  const stream = new ReadableStream(...)
  return stream // Nuxt/Nitro cuida do resto
})
```

**ConclusÃ£o:** Com `httpApi`, o streaming funciona perfeitamente para chat. A Ãºnica limitaÃ§Ã£o real Ã© o **timeout de 30s**.

---

#### ğŸ¯ Como Implementar Streaming: Suas OpÃ§Ãµes

Se vocÃª quer streaming de resposta no chat, tem duas opÃ§Ãµes:

##### âœ… OpÃ§Ã£o 1: httpApi (Atual) - **RECOMENDADO**

**Status:** JÃ¡ configurado e funcionando

```yaml
functions:
  api:
    events:
      - httpApi: '*'
```

**Vantagens:**
- âœ… Streaming jÃ¡ funciona (HTTP Transfer-Encoding: chunked)
- âœ… Zero mudanÃ§as no cÃ³digo
- âœ… PÃºblico por padrÃ£o (sem 403 Forbidden)
- âœ… Setup simples
- âœ… CORS gerenciado pelo middleware (simples)

**Desvantagens:**
- âš ï¸ Timeout de 30s (suficiente para 99% dos casos de chat)

**Como funciona o "streaming":**
- O backend retorna um `ReadableStream` do Nuxt/Nitro
- O API Gateway HTTP API repassa os chunks usando `Transfer-Encoding: chunked`
- O cliente recebe os chunks em tempo real usando `fetch().body.getReader()`
- **NÃ£o Ã© streaming nativo do Lambda**, mas o resultado final Ã© idÃªntico

**MudanÃ§as necessÃ¡rias no cÃ³digo:** **NENHUMA** âœ¨

---

##### âš ï¸ OpÃ§Ã£o 2: Function URL com RESPONSE_STREAM

**Status:** Requer configuraÃ§Ã£o adicional

```yaml
functions:
  api:
    handler: .output/server/index.handler
    url:
      cors: true
      invokeMode: RESPONSE_STREAM
    timeout: 120
    memorySize: 512

# Adicionar no final do serverless.yml
resources:
  Resources:
    ApiLambdaFunctionUrl:
      Type: AWS::Lambda::Url
      Properties:
        AuthType: NONE  # â† Crucial para acesso pÃºblico
        TargetFunctionArn: !GetAtt ApiLambdaFunction.Arn
        InvokeMode: RESPONSE_STREAM
        Cors:
          AllowOrigins: ["*"]
          AllowMethods: ["*"]
          AllowHeaders: ["*"]
    
    ApiLambdaPermissionFnUrl:
      Type: AWS::Lambda::Permission
      Properties:
        FunctionName: !Ref ApiLambdaFunction
        Action: lambda:InvokeFunctionUrl
        Principal: "*"
        FunctionUrlAuthType: NONE
```

**Vantagens:**
- âœ… Streaming nativo da Lambda
- âœ… Timeout atÃ© 15 minutos

**Desvantagens:**
- âŒ Requer configuraÃ§Ã£o CloudFormation extra
- âŒ **CORS deve ser controlado na aplicaÃ§Ã£o** (nÃ£o no `serverless.yml`)
- âŒ Precisa modificar handlers para usar `streamifyResponse`
- âŒ Precisa instalar `@aws-lambda-powertools/streamify`
- âŒ Nuxt/Nitro nÃ£o suporta nativamente `RESPONSE_STREAM` sem adaptaÃ§Ãµes

**MudanÃ§as necessÃ¡rias no cÃ³digo:**

```ts
// ANTES (funciona com httpApi)
export default defineEventHandler(async (event) => {
  const stream = new ReadableStream({
    async start(controller) {
      controller.enqueue('chunk1\n')
      controller.close()
    }
  })
  return stream
})

// DEPOIS (para RESPONSE_STREAM)
import { streamifyResponse } from '@aws-lambda-powertools/streamify'

export const handler = streamifyResponse(async (event, responseStream) => {
  responseStream.write('chunk1\n')
  responseStream.end()
})
```

---

##### ğŸ“Š ComparaÃ§Ã£o de EsforÃ§o

| Aspecto | httpApi (OpÃ§Ã£o 1) | Function URL (OpÃ§Ã£o 2) |
|---------|-------------------|------------------------|
| **Config no serverless.yml** | âœ… JÃ¡ estÃ¡ | âš ï¸ Adicionar resources (20 linhas) |
| **MudanÃ§a no cÃ³digo** | âœ… Zero | âŒ Reescrever todos os handlers |
| **Instalar dependÃªncias** | âœ… Nada | âŒ `pnpm add @aws-lambda-powertools` |
| **Streaming funciona** | âœ… Sim | âœ… Sim |
| **Timeout** | 30s | 15 min |
| **PÃºblico por padrÃ£o** | âœ… Sim | âš ï¸ SÃ³ com CloudFormation |

---

##### ğŸ’¡ RecomendaÃ§Ã£o Final

**Use httpApi (OpÃ§Ã£o 1)** porque:

1. **Streaming jÃ¡ funciona** - seu cÃ³digo atual com `ReadableStream` funciona perfeitamente
2. **30s Ã© suficiente** - respostas de chat GPT raramente excedem 10s
3. **Zero trabalho extra** - nÃ£o precisa mudar absolutamente nada
4. **Simples e confiÃ¡vel** - menos pontos de falha

**SÃ³ use Function URL (OpÃ§Ã£o 2) se:**
- VocÃª realmente precisa de respostas >30s (muito raro)
- Quer experimentar streaming nativo por curiosidade tÃ©cnica
- EstÃ¡ disposto a reescrever todos os handlers

---

#### ğŸš¨ Dificuldades Enfrentadas: HTTP API vs Function URL

Durante o desenvolvimento, enfrentamos diversos desafios ao tentar usar ambos os modos. Aqui estÃ¡ o que aprendemos:

##### 1. CORS: DiferenÃ§as CrÃ­ticas

**HTTP API (httpApi):**
- âœ… CORS pode ser configurado no `serverless.yml` via `provider.httpApi.cors`
- âœ… Middleware global (`server/middleware/api-only.ts`) funciona perfeitamente
- âœ… Headers CORS sÃ£o adicionados automaticamente pelo Gateway
- âœ… Preflight OPTIONS Ã© tratado automaticamente

**Function URL:**
- âš ï¸ `cors: true` no `serverless.yml` **nÃ£o Ã© suficiente**
- âŒ CORS **deve ser controlado manualmente na aplicaÃ§Ã£o**
- âœ… Cada handler precisa adicionar headers CORS explicitamente:
  ```typescript
  setResponseHeader(event, 'Access-Control-Allow-Origin', '*')
  setResponseHeader(event, 'Access-Control-Allow-Headers', 'Content-Type, x-user-id')
  setResponseHeader(event, 'Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
  ```
- âœ… Preflight OPTIONS deve ser tratado manualmente em cada handler:
  ```typescript
  if (event.method === 'OPTIONS') {
    setResponseStatus(event, 204)
    return null
  }
  ```
- âš ï¸ Middleware global nÃ£o Ã© suficiente - cada endpoint precisa de CORS explÃ­cito

**Por que essa diferenÃ§a?**
- HTTP API Gateway processa CORS no nÃ­vel do Gateway (antes de chegar na Lambda)
- Function URL passa tudo direto para a Lambda, incluindo preflight OPTIONS
- Se vocÃª nÃ£o tratar OPTIONS na Lambda, o navegador recebe erro de CORS

##### 2. Streaming: HTTP API Suporta Sim!

**ConfusÃ£o Comum:**
"HTTP API nÃ£o suporta streaming" - **FALSO!**

**Realidade:**
- HTTP API **nÃ£o suporta `RESPONSE_STREAM` nativo do Lambda**
- Mas **suporta streaming via Transfer-Encoding: chunked** (HTTP padrÃ£o)
- Para o cliente (navegador), **nÃ£o hÃ¡ diferenÃ§a**

**Como funciona:**

```typescript
// Seu cÃ³digo (backend) - IGUAL NOS DOIS MODOS
export default defineEventHandler(async (event) => {
  const stream = new ReadableStream({
    async start(controller) {
      for await (const chunk of llmStream) {
        controller.enqueue(encoder.encode(chunk))
      }
      controller.close()
    }
  })
  return stream
})

// Cliente (frontend) - IGUAL NOS DOIS MODOS
const response = await fetch('/api/chat', { method: 'POST', body: ... })
const reader = response.body.getReader()
while (true) {
  const { done, value } = await reader.read()
  if (done) break
  console.log(new TextDecoder().decode(value)) // chunks chegam em tempo real
}
```

**Fluxo Completo:**

| Etapa | HTTP API | Function URL |
|-------|----------|--------------|
| 1. Backend retorna | `ReadableStream` | `ReadableStream` |
| 2. Lambda processa | Chunks â†’ Buffer HTTP | Chunks â†’ RESPONSE_STREAM nativo |
| 3. Gateway/URL repassa | Transfer-Encoding: chunked | Chunks diretos |
| 4. Cliente recebe | Chunks em tempo real âœ… | Chunks em tempo real âœ… |

**Resultado:** Ambos funcionam identicamente para o usuÃ¡rio final!

##### 3. Timeout: A Ãšnica DiferenÃ§a Real

| Modo | Timeout MÃ¡ximo | Suficiente para Chat? |
|------|----------------|----------------------|
| HTTP API | 30 segundos | âœ… Sim (99% dos casos) |
| Function URL | 15 minutos | âœ… Sim (overkill) |

**Quando vocÃª precisaria de >30s:**
- Processamento de documentos muito grandes (>100 pÃ¡ginas)
- MÃºltiplas chamadas encadeadas de APIs externas lentas
- Tarefas de machine learning pesadas

**Para chat RAG:** 30s Ã© mais que suficiente. Respostas tÃ­picas: 2-10s.

##### 4. "Nenhuma resposta gerada" - Causas e SoluÃ§Ãµes

**Quando aparece:**
Esta mensagem Ã© um fallback quando o modelo OpenAI nÃ£o emite nenhum chunk de texto.

**Causas possÃ­veis:**

1. **Nenhum contexto encontrado no Qdrant**
   - **Sintoma:** PDF nÃ£o foi carregado ou foi carregado incorretamente
   - **Como verificar:** Veja logs `[CHAT] Retrieved contexts: 0`
   - **SoluÃ§Ã£o:** Recarregar o PDF via `/api/ingest`

2. **Erro na API OpenAI**
   - **Sintoma:** Chave invÃ¡lida, quota excedida, ou serviÃ§o indisponÃ­vel
   - **Como verificar:** Veja logs `[CHAT] Error:` com detalhes do erro
   - **SoluÃ§Ã£o:** Verificar credenciais, quota, status da OpenAI

3. **Filtragem de conteÃºdo**
   - **Sintoma:** OpenAI bloqueou a resposta por polÃ­tica de conteÃºdo (raro)
   - **Como verificar:** Logs silenciosos, nenhum chunk emitido
   - **SoluÃ§Ã£o:** Reformular a pergunta

4. **Timeout de rede**
   - **Sintoma:** ConexÃ£o OpenAI â†’ Lambda foi interrompida
   - **Como verificar:** Logs param no meio do processo
   - **SoluÃ§Ã£o:** Tentar novamente

**Melhorias implementadas:**

```typescript
// ANTES - mensagem genÃ©rica
if (!emitted) {
  controller.enqueue(encoder.encode('Nenhuma resposta gerada.'))
}

// DEPOIS - mensagem contextual
if (!emitted) {
  const fallbackMessage = contextTexts.length === 0
    ? 'Nenhum contexto relevante foi encontrado no banco de dados para responder sua pergunta. Por favor, verifique se o documento foi carregado corretamente.'
    : 'NÃ£o foi possÃ­vel gerar uma resposta no momento. Por favor, tente reformular sua pergunta ou tente novamente em alguns instantes.'
  controller.enqueue(encoder.encode(fallbackMessage))
}
```

**Logs adicionados para debugging:**

```
[CHAT] Starting chat request processing
[CHAT] User ID: anon
[CHAT] Question: qual meu nome?
[CHAT] History length: 2 messages
[CHAT] Initializing Qdrant client, collection: rag-chatbot-documents
[CHAT] Initializing OpenAI embeddings
[CHAT] Generating query embedding for search
[CHAT] âœ“ Query embedding generated (1536 dimensions)
[CHAT] Searching Qdrant for relevant contexts...
[CHAT] âœ“ Retrieved 4 relevant chunks from Qdrant
[CHAT] Context preview (first chunk): Thiago Silva - Engenheiro de Software...
[CHAT] Preparing conversation history...
[CHAT] âœ“ Converted 2 history messages
[CHAT] Building system prompt with context...
[CHAT] âœ“ System prompt built with 4 total messages
[CHAT] Initializing OpenAI model (gpt-4o-mini) with streaming...
[CHAT] Starting LLM streaming...
[CHAT] âœ… Streaming completed successfully - 47 chunks sent
```

##### 5. Por Que NÃ£o Precisa Recarregar o PDF a Cada SessÃ£o

**Arquitetura de PersistÃªncia:**

```
Cliente (localStorage)     Lambda (stateless)     Qdrant (permanente)
       |                          |                      |
       | 1. Carrega PDF           |                      |
       |------------------------->|                      |
       |                          | 2. Processa chunks   |
       |                          |--------------------->|
       |                          |                      | 3. Salva vetores
       |                          |<---------------------|
       | 4. Confirma OK           |                      |
       |<-------------------------|                      |
       |                          |                      |
       | ... tempo passa ...      |                      |
       |                          |                      |
       | 5. Faz pergunta          |                      |
       |------------------------->|                      |
       |                          | 6. Busca contexto    |
       |                          |--------------------->|
       |                          |<---------------------| 7. Retorna chunks
       |                          | 8. Gera resposta     |
       | 9. Resposta streaming    |                      |
       |<-------------------------|                      |
```

**ConclusÃ£o:** O PDF fica **permanentemente no Qdrant**. VocÃª sÃ³ precisa carregar uma vez.

**Se parece que precisa recarregar:**
- âœ… Verifique se a collection `rag-chatbot-documents` existe no Qdrant
- âœ… Verifique se o upload foi bem-sucedido (logs de ingest)
- âœ… Verifique se a busca estÃ¡ retornando resultados (logs de chat)

---

### ğŸš« Bloqueando Rotas de Frontend no Modo API-Only

Quando vocÃª faz deploy apenas da API, ainda pode acontecer do SSR do Nuxt tentar renderizar pÃ¡ginas HTML. Aqui estÃ¡ como bloquear completamente.

#### âœ… Resultado Esperado

```bash
# âŒ Rota raiz bloqueada
$ curl https://sua-api.execute-api.sa-east-1.amazonaws.com
{"message":"Not Found"}  # 404 do API Gateway

# âœ… Rotas /api/* funcionam
$ curl https://sua-api.execute-api.sa-east-1.amazonaws.com/api/chat
{"message":"Chat endpoint - a ser implementado"}  # JSON puro
```

---

#### ğŸ”§ SoluÃ§Ã£o 1: Bloquear no API Gateway (RECOMENDADO)

Configure rotas especÃ­ficas no `serverless.yml`:

```yaml
functions:
  api:
    handler: .output/server/index.handler
    events:
      # Bloquear rotas de frontend - sÃ³ permite /api/*
      - httpApi:
          path: /api/{proxy+}
          method: ANY
      - httpApi:
          path: /api/chat
          method: POST
      - httpApi:
          path: /api/ingest
          method: POST
```

**Vantagem:** O API Gateway retorna **404 antes de chegar na Lambda** â†’ economia de custo e latÃªncia.

---

#### ğŸ”§ SoluÃ§Ã£o 2: ForÃ§ar Content-Type nos Handlers

Adicione `setResponseHeader` em **todos os endpoints**:

```typescript
// server/api/chat.post.ts
export default defineEventHandler(async (event) => {
  // ForÃ§ar resposta JSON (evita SSR renderizar HTML)
  setResponseHeader(event, 'Content-Type', 'application/json')
  
  return {
    message: 'Chat endpoint - a ser implementado'
  }
})
```

**Por que precisa disso?**

O Nuxt tem um **sistema de rotas universal** que tenta renderizar pÃ¡ginas HTML mesmo para rotas `/api/*` quando vocÃª retorna um objeto simples. Ao definir `Content-Type: application/json`, vocÃª forÃ§a o Nuxt a enviar JSON puro sem passar pelo SSR.

---

#### ğŸ”§ SoluÃ§Ã£o 3: Middleware Global (Opcional)

Adicione um middleware que bloqueia tudo exceto `/api/*`:

```typescript
// server/middleware/api-only.ts
export default defineEventHandler((event) => {
  const path = event.path || event.node.req.url || ''
  
  // Permitir apenas rotas /api/*
  if (!path.startsWith('/api/')) {
    throw createError({
      statusCode: 404,
      statusMessage: 'Not Found',
      message: 'API-only mode. Frontend runs locally in Docker.'
    })
  }
})
```

**Vantagem:** Bloqueia globalmente, mas a Lambda ainda processa a request.

---

#### ğŸ“Š Qual usar?

| Abordagem | Bloqueia Onde | Vantagem | Desvantagem |
|-----------|---------------|----------|-------------|
| **SoluÃ§Ã£o 1: httpApi paths** | API Gateway | NÃ£o chega na Lambda | Precisa listar todas as rotas |
| **SoluÃ§Ã£o 2: Content-Type** | Handler | Simples, 1 linha | Precisa em todos os endpoints |
| **SoluÃ§Ã£o 3: Middleware** | Nitro/Nuxt | Global, 3 linhas | Lambda processa antes de bloquear |

**RecomendaÃ§Ã£o:** **SoluÃ§Ã£o 1 + 2** (bloquear no Gateway + forÃ§ar JSON nos handlers).

---

### ğŸ”§ BÃ´nus: O que o Build do Nuxt/Nitro faz automaticamente

Embora o problema tenha sido no `serverless.yml`, Ã© Ãºtil entender que o **Nitro jÃ¡ otimiza** o build automaticamente:

#### OtimizaÃ§Ãµes automÃ¡ticas do Nitro:
- âœ… Remove `test/`, `docs/`, `examples/` folders
- âœ… Remove `*.md`, `*.map` files
- âœ… Faz tree-shaking de cÃ³digo nÃ£o usado
- âœ… Minifica o cÃ³digo JavaScript
- âœ… Bundla dependÃªncias em um Ãºnico arquivo (`index.mjs`)

#### ConfiguraÃ§Ãµes extras no `nuxt.config.ts`:

```typescript
nitro: {
  preset: 'aws-lambda',
  serveStatic: false,
  minify: true,
  sourcemap: false,  // Remove source maps
  
  rollupConfig: {
    output: {
      format: 'esm',
      sourcemap: false
    }
  }
}
```

**Resultado do Build:**
```
.output/server/
â”œâ”€â”€ index.mjs       (208 KB) â† CÃ³digo bundlado
â”œâ”€â”€ package.json    (673 B)
â””â”€â”€ node_modules/   (~1.5 MB) â† Apenas deps nÃ£o bundlÃ¡veis
```

> **Importante:** Essas otimizaÃ§Ãµes do Nitro **sempre funcionaram**. O problema de 130 MB era porque o Serverless Framework incluÃ­a o `node_modules/` da **raiz do projeto** (500 MB), nÃ£o o `.output/server/node_modules/` otimizado.

---

#### âš™ï¸ ConfiguraÃ§Ã£o Final do serverless.yml

```yaml
package:
  patterns:
    - '!**'                     # 1. Exclui TUDO (primeira regra)
    - '!node_modules/**'        # 2. Garante exclusÃ£o de node_modules raiz
    - '!.nuxt/**'               # 3. Exclui cache do Nuxt
    - '!.git/**'                # 4. Exclui arquivos git
    - '.output/server/index.mjs'        # 5. Inclui sÃ³ o que precisa
    - '.output/server/package.json'
    - '.output/server/node_modules/**'  # 6. Inclui deps do build
```

**Por que isso funciona?**
1. `'!**'` exclui **absolutamente tudo** do projeto
2. Depois vocÃª adiciona de volta **apenas** os arquivos da pasta `.output/server/`
3. O `node_modules/` da raiz (500 MB) **nunca Ã© incluÃ­do**
4. Apenas o `node_modules/` otimizado dentro de `.output/server/` (2 MB) Ã© incluÃ­do

**Resultado real do projeto:**
```
Antes (ordem errada): 130 MB ZIP âŒ (incluÃ­a node_modules raiz)
Depois (ordem correta): 517 KB ZIP âœ… (apenas .output/server)
```

---

#### ğŸ“¦ OtimizaÃ§Ã£o de package.json: DevDependencies

Outra dica importante Ã© garantir que suas **dependÃªncias de desenvolvimento** estejam em `devDependencies`, nÃ£o em `dependencies`.

##### ğŸ¤” Por que isso importa?

Quando vocÃª roda `pnpm install --production` ou quando o Serverless Framework faz o bundle, ele **ignora** tudo que estÃ¡ em `devDependencies`. Isso reduz o tamanho final do pacote.

##### âœ… DependÃªncias que devem estar em `devDependencies`:

| Pacote | Por quÃª? |
|--------|----------|
| `dotenv` | Usado apenas em desenvolvimento (variÃ¡veis vÃ£o pro env da Lambda) |
| `serverless` | Ferramenta de CLI, nÃ£o roda em produÃ§Ã£o |
| `serverless-dotenv-plugin` | Plugin do Serverless, nÃ£o vai pro Lambda |
| `typescript` | Compilador, cÃ³digo final Ã© JS |
| `eslint`, `prettier` | Ferramentas de dev |
| `nodemon` | Dev server, nÃ£o usa em Lambda |
| `@types/*` | Types do TypeScript |
| `tsx` | Runtime de dev para TypeScript |

##### âœ… DependÃªncias que devem estar em `dependencies`:

| Pacote | Por quÃª? |
|--------|----------|
| `langchain`, `@langchain/*` | Usado em runtime |
| `openai` | Cliente da API OpenAI |
| `@qdrant/js-client-rest` | Cliente do Qdrant |
| `nuxt`, `vue` | Framework de runtime |
| `uuid` | Usado em runtime |

##### ğŸ“‹ Exemplo de package.json correto:

```json
{
  "dependencies": {
    "langchain": "^0.3.x",
    "@langchain/openai": "^0.5.x",
    "nuxt": "^3.x",
    "vue": "^3.x"
  },
  "devDependencies": {
    "dotenv": "^16.x",
    "serverless": "^3.x",
    "serverless-dotenv-plugin": "^6.x",
    "typescript": "^5.x",
    "@types/node": "^22.x"
  }
}
```

> **ğŸ’¡ Dica:** Execute `pnpm install` apÃ³s mover dependÃªncias para garantir que o `pnpm-lock.yaml` seja atualizado corretamente.

---

### ğŸ”— Alternativa: Lambda Layers

Se as otimizaÃ§Ãµes acima nÃ£o forem suficientes, a alternativa Ã© usar **Lambda Layers** para separar as dependÃªncias pesadas.

#### ğŸ¤” O que sÃ£o Lambda Layers?

Layers sÃ£o pacotes de cÃ³digo/dependÃªncias compartilhadas que podem ser reutilizadas por mÃºltiplas Lambda Functions. VocÃª sobe as deps uma vez e referencia em vÃ¡rias funÃ§Ãµes.

#### ğŸ“¦ Estrutura com Layers

```
meu-projeto/
â”œâ”€â”€ lambda-function/     (seu cÃ³digo - 5 MB)
â”‚   â”œâ”€â”€ index.mjs
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ langchain-layer/         (Layer 1: 50 MB)
â”‚   â”‚   â”œâ”€â”€ nodejs/node_modules/
â”‚   â”‚   â”‚   â”œâ”€â”€ langchain/
â”‚   â”‚   â”‚   â”œâ”€â”€ @langchain/
â”‚   â”‚   â”‚   â””â”€â”€ encoding/
â”‚   â”‚   â””â”€â”€ layer.zip
â”‚   â”‚
â”‚   â””â”€â”€ openai-layer/            (Layer 2: 40 MB)
â”‚       â”œâ”€â”€ nodejs/node_modules/
â”‚       â”‚   â”œâ”€â”€ openai/
â”‚       â”‚   â””â”€â”€ axios/
â”‚       â””â”€â”€ layer.zip
â”‚
â””â”€â”€ serverless.yml  (referencia as layers)
```

#### âœ… Vantagens de Usar Layers

1. **SeparaÃ§Ã£o de preocupaÃ§Ãµes**: Cada layer tem responsabilidade clara
2. **ReutilizaÃ§Ã£o**: Mesma layer pode ser usada por mÃºltiplas funÃ§Ãµes
3. **Versionamento**: VocÃª versiona cada layer independentemente
4. **Tamanho menor**: FunÃ§Ã£o fica ~5 MB, layers compartilhadas (uploaded uma vez)
5. **Cold start mais rÃ¡pido**: Sem precisar descompactar 200+ MB
6. **AtualizaÃ§Ã£o independente**: Atualizar dependÃªncia nÃ£o redeploiya a funÃ§Ã£o inteira

#### âŒ Desvantagens

1. **Complexidade aumenta**: Mais configuraÃ§Ã£o no `serverless.yml`
2. **Limite de layers**: AWS permite atÃ© 5 layers por funÃ§Ã£o
3. **EspaÃ§o total**: Soma da funÃ§Ã£o + todas as layers nÃ£o pode exceder 250 MB descompactado
4. **Deployment mais lento**: Precisa fazer upload de mÃºltiplos ZIPs

#### ğŸ“‹ ImplementaÃ§Ã£o com Layers (Exemplo)

```yaml
# serverless.yml com Layers

service: rag-chatbot-mvp

plugins:
  - serverless-dotenv-plugin

provider:
  name: aws
  runtime: nodejs18.x
  region: sa-east-1

# ğŸ”¥ Definir Layers
layers:
  langchainLayer:
    path: layers/langchain
    name: rag-chatbot-langchain-${sls:stage}
    
  openaiLayer:
    path: layers/openai
    name: rag-chatbot-openai-${sls:stage}

functions:
  api:
    handler: .output/server/index.handler
    # ğŸ”¥ Referenciar layers
    layers:
      - !Ref LangchainLayerLambdaLayer
      - !Ref OpenaiLayerLambdaLayer
    url:
      cors: true
      invokeMode: RESPONSE_STREAM
    timeout: 120
    memorySize: 512
```

#### ğŸ› ï¸ Como Criar uma Layer

```bash
# 1. Criar estrutura de diretÃ³rios
mkdir -p layers/langchain/nodejs

# 2. Navegar e instalar deps
cd layers/langchain/nodejs
pnpm init -y
pnpm add langchain @langchain/core @langchain/community @langchain/openai

# 3. Compactar (a estrutura AWS espera nodejs/node_modules)
cd ..
zip -r langchain-layer.zip nodejs/

# 4. No serverless.yml, apontar para o ZIP
# (o path aponta para o arquivo ZIP ou para o diretÃ³rio)
```

#### ğŸ“Š Tamanho Comparativo

```
SEM Layers (Current):
â”œâ”€â”€ FunÃ§Ã£o ZIP: 130 MB âŒ Excede limite

COM Layers (Otimizado):
â”œâ”€â”€ FunÃ§Ã£o ZIP: 5 MB
â”œâ”€â”€ Langchain Layer: 50 MB (reutilizÃ¡vel)
â””â”€â”€ OpenAI Layer: 40 MB (reutilizÃ¡vel)
Total enviado: 95 MB (layers sÃ£o enviadas apenas uma vez) âœ…
```

#### ğŸ¯ Quando Usar Layers

- **Use Layers se:**
  - Bundle principal > 50 MB mesmo com otimizaÃ§Ãµes
  - VocÃª tem mÃºltiplas funÃ§Ãµes Lambda reutilizando as mesmas deps
  - Quer cold starts mais rÃ¡pidos
  - Precisa versionar dependÃªncias independentemente

- **NÃ£o use Layers se:**
  - Bundle < 50 MB (simples Ã© melhor)
  - FunÃ§Ã£o Ã© Ãºnica e nÃ£o Ã© reutilizada
  - Quer keep it simple para MVP

---

## 7. Estrutura do Projeto

```
Serverless-RAG-Chatbot-MVP-/
â”œâ”€â”€ .env.example                 # Template de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore                   # Arquivos ignorados pelo Git
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ package.json                 # DependÃªncias do projeto
â”œâ”€â”€ nuxt.config.ts               # ConfiguraÃ§Ã£o do Nuxt 3
â”œâ”€â”€ tsconfig.json                # ConfiguraÃ§Ã£o do TypeScript
â”œâ”€â”€ tailwind.config.js           # ConfiguraÃ§Ã£o do Tailwind CSS
â”œâ”€â”€ serverless.yml               # ConfiguraÃ§Ã£o de deploy AWS Lambda
â”‚
â”œâ”€â”€ app.vue                      # Componente raiz da aplicaÃ§Ã£o
â”œâ”€â”€ pages/                       # PÃ¡ginas da aplicaÃ§Ã£o
â”‚   â””â”€â”€ index.vue                # PÃ¡gina inicial (Chat UI)
â”‚
â”œâ”€â”€ components/                  # Componentes Vue reutilizÃ¡veis
â”‚   â”œâ”€â”€ ChatMessage.vue          # Componente de mensagem individual
â”‚   â”œâ”€â”€ ChatInput.vue            # Componente de input de mensagem
â”‚   â”œâ”€â”€ ChatWindow.vue           # Componente da janela de chat
â”‚   â”œâ”€â”€ ThemeToggle.vue          # Toggle de dark mode
â”‚   â””â”€â”€ DocumentUpload.vue       # Upload de PDF (drag & drop)
â”‚
â”œâ”€â”€ composables/                 # Composables Vue (lÃ³gica reutilizÃ¡vel)
â”‚   â”œâ”€â”€ useChatHistory.ts        # Gerenciamento do histÃ³rico no LocalStorage
â”‚   â””â”€â”€ useChatStream.ts         # LÃ³gica de streaming de mensagens
â”‚
â”œâ”€â”€ server/                      # Backend Nitro
â”‚   â”œâ”€â”€ api/                     # Rotas da API
â”‚   â”‚   â”œâ”€â”€ chat.post.ts         # Endpoint de conversaÃ§Ã£o (streaming)
â”‚   â”‚   â””â”€â”€ ingest.post.ts       # Endpoint de upload de PDF
â”‚   â”œâ”€â”€ utils/                   # UtilitÃ¡rios do servidor
â”‚   â”‚   â”œâ”€â”€ langchain.ts         # ConfiguraÃ§Ã£o do LangChain
â”‚   â”‚   â””â”€â”€ qdrant.ts            # ConfiguraÃ§Ã£o do Qdrant
â”‚   â””â”€â”€ middleware/              # Middleware do servidor
â”‚
â”œâ”€â”€ scripts/                     # Scripts auxiliares
â”‚   â”œâ”€â”€ ingest.ts                # Script de ingestÃ£o de PDF para Qdrant
â”‚   â””â”€â”€ README.md                # DocumentaÃ§Ã£o dos scripts
â”‚
â”œâ”€â”€ data/                        # Dados do projeto
â”‚   â””â”€â”€ documents/               # PDFs para ingestÃ£o
â”‚       â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ public/                      # Arquivos estÃ¡ticos
â”‚   â””â”€â”€ favicon.ico
â”‚
â””â”€â”€ assets/                      # Assets processados (CSS, imagens)
    â””â”€â”€ css/
        â””â”€â”€ main.css             # CSS global
```

---

## 8. HTTP API vs REST API no AWS API Gateway

### DiferenÃ§as Principais

#### HTTP API (Usado no Projeto)
- **Mais novo**: LanÃ§ado em 2019
- **Mais barato**: AtÃ© 70% mais barato que REST API
- **Mais rÃ¡pido**: Menor latÃªncia
- **Mais simples**: Menos features, mais focado
- **LimitaÃ§Ãµes**:
  - NÃ£o suporta `multipart/form-data` nativamente
  - BinÃ¡rios sÃ£o automaticamente codificados em base64
  - Menos opÃ§Ãµes de autorizaÃ§Ã£o
  - Sem API Keys nativas

#### REST API (VersÃ£o Antiga)
- **Mais antigo**: DisponÃ­vel desde o inÃ­cio
- **Mais caro**: PreÃ§o mais alto por requisiÃ§Ã£o
- **Mais features**: Suporte completo a binÃ¡rios, API Keys, etc.
- **Mais complexo**: Mais opÃ§Ãµes de configuraÃ§Ã£o
- **Vantagens**:
  - Suporta `multipart/form-data` nativamente
  - NÃ£o codifica binÃ¡rios automaticamente
  - API Keys integradas
  - Mais opÃ§Ãµes de autorizaÃ§Ã£o

### Por Que HTTP API Codifica em Base64?

Quando vocÃª usa HTTP API e envia dados binÃ¡rios (como PDF):

1. **API Gateway detecta dados binÃ¡rios**
2. **Codifica automaticamente em base64**
3. **Define `event.isBase64Encoded = true`**
4. **Passa para Lambda jÃ¡ codificado**

### Exemplo do Fluxo

```
Cliente â†’ PDF (binÃ¡rio) â†’ API Gateway â†’ Base64 â†’ Lambda â†’ Precisa decodificar
```

### SoluÃ§Ã£o Correta para HTTP API

#### âŒ SoluÃ§Ã£o ERRADA (mudar para REST API)
```yaml
events:
  - http:  # â† REST API
      path: /api/ingest
      method: POST
```

**Problemas**:
- Mais caro (70% mais caro)
- Perde benefÃ­cios de performance
- NÃ£o resolve o problema real

#### âœ… SoluÃ§Ã£o CORRETA (decodificar base64)

**No Cliente** (opcional - pode enviar binÃ¡rio normal):
```typescript
// OpÃ§Ã£o 1: Enviar binÃ¡rio direto (deixar API Gateway codificar)
const formData = new FormData()
formData.append('file', pdfFile)
await fetch('/api/ingest', { method: 'POST', body: formData })

// OpÃ§Ã£o 2: Codificar manualmente (mais controle)
const base64 = Buffer.from(pdfBuffer).toString('base64')
await fetch('/api/ingest', {
  method: 'POST',
  body: JSON.stringify({ body: base64 }),
  headers: { 'Content-Type': 'application/json' }
})
```

**Na Lambda** (sempre necessÃ¡rio com HTTP API):
```typescript
// Detectar se estÃ¡ codificado em base64
if (event.isBase64Encoded) {
  // Decodificar antes de processar
  const buffer = Buffer.from(event.body, 'base64')
  // Processar buffer decodificado
}
```

### ImplementaÃ§Ã£o no Nuxt com h3

#### Problema Identificado

No nosso caso, usamos `readMultipartFormData` do h3, que nÃ£o sabe que o API Gateway jÃ¡ codificou em base64. EntÃ£o:

1. Cliente envia PDF binÃ¡rio
2. API Gateway codifica em base64
3. h3 recebe base64 mas trata como binÃ¡rio
4. Resultado: dados corrompidos (650893 bytes ao invÃ©s de 373035)

#### SoluÃ§Ã£o

```typescript
import { defineEventHandler, readRawBody } from 'h3'

export default defineEventHandler(async (event) => {
  // Obter o evento Lambda original
  const lambdaEvent = event.node.req

  // Verificar se estÃ¡ codificado em base64
  if (lambdaEvent.isBase64Encoded) {
    // Ler o body raw e decodificar
    const rawBody = await readRawBody(event)
    const decodedBody = Buffer.from(rawBody, 'base64')
    
    // Agora processar o multipart do body decodificado
    // ... parsear multipart manualmente ou usar biblioteca
  } else {
    // Processar normalmente
    const body = await readMultipartFormData(event)
  }
})
```

### ComparaÃ§Ã£o de Custos

#### CenÃ¡rio: 1 milhÃ£o de requests/mÃªs

**HTTP API**:
- Custo: ~$1.00/milhÃ£o requests
- Total: $1.00/mÃªs

**REST API**:
- Custo: ~$3.50/milhÃ£o requests
- Total: $3.50/mÃªs

**Economia com HTTP API**: 70% ($2.50/mÃªs)

### Quando Usar Cada Um?

#### Use HTTP API quando:
- âœ… Custo Ã© prioridade
- âœ… Performance Ã© crÃ­tica
- âœ… NÃ£o precisa de features avanÃ§adas
- âœ… Pode implementar encoding/decoding
- âœ… API simples REST/GraphQL

#### Use REST API quando:
- âœ… Precisa de API Keys nativas
- âœ… Precisa de authorizers complexos
- âœ… Precisa de request/response transformation
- âœ… NÃ£o quer lidar com base64
- âœ… Usa muito multipart/form-data

### RecomendaÃ§Ã£o para o Projeto

**Manter HTTP API** por:
1. **Custo**: 70% mais barato
2. **Performance**: Menor latÃªncia
3. **Modernidade**: Arquitetura mais nova
4. **SoluÃ§Ã£o Simples**: Apenas decodificar base64

A soluÃ§Ã£o de decodificaÃ§Ã£o base64 Ã© simples e resolve o problema completamente, mantendo todos os benefÃ­cios do HTTP API.

### ReferÃªncias

- [AWS HTTP APIs vs REST APIs](https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html)
- [Working with Binary Media Types](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings.html)
- [StackOverflow: Post PDF to AWS Lambda](https://stackoverflow.com/questions/57121011/how-can-i-post-a-pdf-to-aws-lambda)

---

## 9. Problemas Identificados na Rota de IngestÃ£o de PDF

### Data: 2025-12-10

#### Resumo Executivo
Durante os testes da rota `/api/ingest` com o arquivo `thiago_relatorio.pdf`, foram identificados diversos problemas relacionados ao parsing de PDF no ambiente AWS Lambda e ao tratamento de dados multipart/form-data.

---

### 1. Lambda nÃ£o conseguiu ler o PDF - Necessidade de salvar em /tmp

#### Problema
O PDFLoader do LangChain nÃ£o conseguiu processar o PDF diretamente do Buffer recebido via multipart/form-data. Foi necessÃ¡rio salvar temporariamente em `/tmp`.

#### Causa
- O PDFLoader espera um caminho de arquivo ou Blob vÃ¡lido
- Buffer recebido via h3's `readMultipartFormData` nÃ£o Ã© diretamente compatÃ­vel com PDFLoader
- No ambiente Lambda, o sistema de arquivos Ã© read-only exceto `/tmp`

#### Tentativas de SoluÃ§Ã£o
1. **Tentativa 1**: Converter Buffer para Blob
   ```typescript
   const pdfBlob = new Blob([filePart.data], { type: 'application/pdf' })
   const loader = new PDFLoader(pdfBlob)
   ```
   **Resultado**: Falhou com erro de tipo

2. **Tentativa 2**: Converter Buffer para Uint8Array antes do Blob
   ```typescript
   const pdfBlob = new Blob([new Uint8Array(filePart.data)], { type: 'application/pdf' })
   ```
   **Resultado**: Falhou com mesmo erro

3. **SoluÃ§Ã£o Final**: Salvar temporariamente em `/tmp`
   ```typescript
   const tempFilePath = join('/tmp', `upload-${Date.now()}.pdf`)
   writeFileSync(tempFilePath, pdfBuffer)
   const loader = new PDFLoader(tempFilePath)
   ```
   **Resultado**: PDF carregado com sucesso, mas com problemas de parsing

---

### 2. Problema de Parsing do PDF - Biblioteca pdf-parse

#### Problema
A biblioteca PDFLoader do LangChain usa pdf-parse internamente, que estava gerando warnings de stream corrompido:
```
Warning: Invalid stream: "FormatError: Bad FCHECK in flate stream: 120, 239"
```

#### Causa
- O PDF `thiago_relatorio.pdf` tem streams de compressÃ£o que o pdf-parse interpreta como corrompidos
- Localmente funciona perfeitamente (7516 caracteres extraÃ­dos)
- No Lambda, apenas 10 caracteres (quebras de linha) foram extraÃ­dos

#### Logs
```
2025-12-10 15:21:08.793	INFO	PDF buffer size: 650893
2025-12-10 15:21:09.530	INFO	PDF pages: 5
2025-12-10 15:21:09.530	INFO	Text length: 10
```

#### Tentativas de SoluÃ§Ã£o
1. Usar PDFLoader com opÃ§Ãµes padrÃ£o
2. Usar pdf-parse diretamente com opÃ§Ãµes customizadas
3. Ambas falharam no ambiente Lambda

---

### 3. Collection do Qdrant NÃ£o Existia

#### Problema
A collection `rag-chatbot-documents` nÃ£o existia no Qdrant Cloud, causando erro `Not Found`.

#### Causa
- Collection precisa ser criada antes de tentar fazer upsert
- Qdrant nÃ£o cria collections automaticamente

#### SoluÃ§Ã£o
Script de teste executado:
```javascript
const qdrant = new QdrantClient({
  url: process.env.QDRANT_URL,
  apiKey: process.env.QDRANT_API_KEY
});

await qdrant.createCollection('rag-chatbot-documents', {
  vectors: {
    size: 1536, // OpenAI embeddings dimension
    distance: 'Cosine'
  }
});
```

**Resultado**: Collection criada com sucesso

---

### 4. Erro "No text content found in PDF"

#### Problema
ApÃ³s resolver o problema da collection, o erro mudou para "No text content found in PDF".

#### Causa
- PDF estava sendo processado, mas apenas 10 caracteres (quebras de linha) foram extraÃ­dos
- Problema especÃ­fico do ambiente Lambda com pdf-parse
- Localmente: 7516 caracteres extraÃ­dos corretamente
- Lambda: 10 caracteres (apenas `\n\n\n\n\n\n\n\n\n\n`)

#### Logs Comparativos
**Local**:
```
âœ… PDF parsed successfully!
Pages: 5
Text length: 7516
```

**Lambda**:
```
INFO	PDF pages: 5
INFO	Text length: 10
```

---

### 5. Erro ao Ler Buffer do PDF no Lambda

#### Problema
O buffer estava chegando com tamanho incorreto: **650893 bytes** ao invÃ©s de **373035 bytes** (quase o dobro).

#### Causa
- O multipart/form-data estava incluindo metadados adicionais no buffer
- h3's `readMultipartFormData` retorna dados que podem incluir boundaries e headers

#### Logs
```
INFO	PDF buffer size: 650893
INFO	First 10 bytes: 255044462d312e370d0a  (%PDF-1.7)
INFO	Last 10 bytes: 3835320d0a2525454f46  (%%EOF)
```

**ObservaÃ§Ã£o**: Apesar dos primeiros e Ãºltimos bytes estarem corretos (`%PDF-1.7` e `%%EOF`), o tamanho total estava incorreto.

#### Tentativas de SoluÃ§Ã£o
1. **Tentativa 1**: Extrair PDF do buffer buscando `%PDF` e `%%EOF`
   ```typescript
   const pdfStart = pdfBuffer.indexOf('%PDF')
   const pdfEnd = pdfBuffer.lastIndexOf('%%EOF')
   if (pdfStart !== -1 && pdfEnd !== -1) {
     pdfBuffer = pdfBuffer.slice(pdfStart, pdfEnd + 5)
   }
   ```
   **Resultado**: Tamanho continuou 650893 bytes

---

### 6. Erro com Multipart Form Data - Metadados do filePart

#### Problema
O `filePart.data` do multipart form data contÃ©m mais dados do que apenas o PDF.

#### Causa Raiz Identificada
ApÃ³s anÃ¡lise do StackOverflow (https://stackoverflow.com/questions/57121011/how-can-i-post-a-pdf-to-aws-lambda), descobrimos que:

1. **API Gateway HTTP API vs REST API**:
   - HTTP API nÃ£o suporta `multipart/form-data` nativamente
   - Dados sÃ£o codificados em base64 automaticamente
   - Precisa decodificar antes de usar

2. **Problema com h3's `readMultipartFormData`**:
   - Pode estar recebendo dados jÃ¡ transformados pelo API Gateway
   - Buffer pode conter encoding adicional

#### Logs de Debug
```typescript
console.log('File part:', {
  name: filePart.name,        // 'file'
  filename: filePart.filename, // 'thiago_relatorio.pdf'
  type: filePart.type,        // 'application/pdf'
  dataLength: 650893,         // âŒ Deveria ser 373035
  dataType: 'object',
  isBuffer: true
})
```

---

### SoluÃ§Ã£o Proposta pelo StackOverflow

#### Problema Principal
API Gateway HTTP API nÃ£o suporta `multipart/form-data` diretamente. Quando vocÃª faz POST de um PDF:

1. API Gateway detecta binÃ¡rio
2. Codifica em base64 automaticamente
3. Passa para Lambda jÃ¡ codificado
4. Ã‰ necessÃ¡rio decodificar na Lambda

#### SoluÃ§Ã£o Correta

**OpÃ§Ã£o 1: Usar REST API ao invÃ©s de HTTP API**
```yaml
# serverless.yml
functions:
  api:
    handler: .output/server/index.handler
    events:
      - http:  # â† REST API ao invÃ©s de httpApi
          path: /api/ingest
          method: POST
```

**OpÃ§Ã£o 2: Decodificar base64 na Lambda**
```typescript
// Detectar se Ã© base64
if (event.isBase64Encoded) {
  const buffer = Buffer.from(event.body, 'base64')
  // Parsear multipart manualmente ou usar biblioteca como busboy
}
```

**OpÃ§Ã£o 3: Usar S3 Presigned URL (Recomendado para arquivos grandes)**
1. Cliente solicita presigned URL
2. Cliente faz upload direto para S3
3. Lambda Ã© trigada por evento S3
4. Lambda processa arquivo de S3

---

### Status Atual dos Testes

#### âœ… Funcionando
- Deploy da Lambda
- Connection com Qdrant
- Collection criada
- VariÃ¡veis de ambiente configuradas
- Rota acessÃ­vel
- Upload do arquivo sendo recebido

#### âŒ NÃ£o Funcionando (ANTES da soluÃ§Ã£o)
- Parsing correto do PDF (10 chars ao invÃ©s de 7516)
- Tamanho correto do buffer (650893 ao invÃ©s de 373035)
- ExtraÃ§Ã£o de texto completo

#### ğŸ“Š EstatÃ­sticas (ANTES da soluÃ§Ã£o)
- **PDF Original**: 373035 bytes, 5 pÃ¡ginas, 7516 caracteres
- **Buffer Recebido**: 650893 bytes (174% maior)
- **Texto ExtraÃ­do**: 10 caracteres (0.13% do esperado)
- **Taxa de Sucesso Local**: 100%
- **Taxa de Sucesso Lambda**: 0%

#### âœ… Status DEPOIS da SoluÃ§Ã£o Base64
- **Buffer Recebido**: 373035 bytes âœ… (correto)
- **Texto ExtraÃ­do**: 7516 caracteres âœ… (correto)
- **Taxa de Sucesso Lambda**: 100% âœ…

---

### PrÃ³ximos Passos

1. âœ… **RESOLVIDO**: Implementar soluÃ§Ã£o do StackOverflow (base64)
2. Testar com PDF mais simples sem compressÃ£o
3. Considerar usar S3 Presigned URL para produÃ§Ã£o
4. Adicionar logging mais detalhado do processo multipart
5. Implementar fallback para diferentes formatos de PDF

---

### 4. CORS para ingestÃ£o via frontend

Para permitir que o frontend local (http://localhost:3000) chame a Lambda em `API_BASE_URL`, configuramos CORS diretamente no handler de ingestÃ£o (`server/api/ingest.post.ts`):
- `Access-Control-Allow-Origin: *`
- `Access-Control-Allow-Methods: POST, OPTIONS`
- `Access-Control-Allow-Headers: Content-Type`
- Resposta imediata para `OPTIONS` com `OK`.

Assim, basta definir `API_BASE_URL` apontando para a URL da API e o upload funciona sem ajustes adicionais no frontend.

### 5. Problema de Formato de Point ID no Qdrant

#### Problema
Ao tentar fazer upload de PDFs via rota `/api/ingest`, a API retornava erro `Bad Request` do Qdrant:
```
ApiError: Bad Request
Format error in JSON body: value test-user-python-1765402104281-0 is not a valid point ID, 
valid values are either an unsigned integer or a UUID
```

#### Causa
O cÃ³digo estava gerando IDs de pontos (vectors) no formato `${documentId}-${index}` (ex: `test-user-python-1765402104281-0`), mas o **Qdrant requer que Point IDs sejam exclusivamente UUIDs ou nÃºmeros inteiros nÃ£o assinados**. Strings arbitrÃ¡rias nÃ£o sÃ£o aceitas.

#### SoluÃ§Ã£o Implementada
Foi adicionado o import da biblioteca `uuid` e alterado o cÃ³digo para gerar UUIDs vÃ¡lidos para cada chunk:

```typescript
import { v4 as uuidv4 } from 'uuid'

// Antes (ERRADO):
const points = chunks.map((chunk, idx) => ({
  id: `${documentId}-${idx}`, // âŒ Formato invÃ¡lido
  vector: vectors[idx],
  payload: { ... }
}))

// Depois (CORRETO):
const points = chunks.map((chunk, idx) => ({
  id: uuidv4(), // âœ… UUID vÃ¡lido
  vector: vectors[idx],
  payload: { ... }
}))
```

#### Como Identificar o Problema
1. **Logs da Lambda**: Use `serverless logs -f api --startTime 10m` para ver os erros detalhados
2. **Teste Local**: Execute `pnpm dev` e teste a rota localmente - o erro aparece igual
3. **Mensagem de Erro**: O Qdrant retorna explicitamente o formato esperado na mensagem de erro

#### Status
âœ… **RESOLVIDO** - A rota `/api/ingest` agora funciona corretamente tanto localmente quanto no Lambda.

---

### LiÃ§Ãµes Aprendidas

1. **API Gateway HTTP API vs REST API**: DiferenÃ§as crÃ­ticas no tratamento de binÃ¡rios
2. **Multipart no Lambda**: Requer tratamento especial
3. **pdf-parse no Lambda**: SensÃ­vel a compressÃ£o de PDF
4. **Testes Locais vs Lambda**: Comportamento diferente com bibliotecas de parsing
5. **Qdrant**: Collection deve ser criada previamente
6. **Buffer Size**: Verificar sempre o tamanho real vs esperado
7. **Qdrant Point IDs**: Devem ser UUIDs ou inteiros, nunca strings arbitrÃ¡rias
7. **SoluÃ§Ã£o Base64**: Enviar PDF como JSON com base64 resolve completamente o problema

---

### ReferÃªncias

- [StackOverflow: How can I post a PDF to AWS lambda](https://stackoverflow.com/questions/57121011/how-can-i-post-a-pdf-to-aws-lambda)
- [AWS API Gateway Binary Support](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings.html)
- [Serverless Framework HTTP vs HTTP API](https://www.serverless.com/framework/docs/providers/aws/events/http-api)

---

## 10. Processamento de PDF: LangChain 100% vs Abordagem HÃ­brida

### Contexto

Para processar PDFs e gerar embeddings para RAG, existem duas abordagens principais:
1. **LangChain 100%**: Usar `PDFLoader` do LangChain para tudo
2. **Abordagem HÃ­brida**: Usar `pdf-parse` para extraÃ§Ã£o + LangChain para chunks/embeddings

Este projeto usa a **Abordagem HÃ­brida**. Aqui estÃ¡ o porquÃª.

---

### Abordagem 1: LangChain 100% (PDFLoader)

```typescript
import { PDFLoader } from 'langchain/document_loaders/fs/pdf'
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter'
import { OpenAIEmbeddings } from '@langchain/openai'

// 1. Salvar em /tmp (obrigatÃ³rio)
const tempFilePath = join('/tmp', `upload-${Date.now()}.pdf`)
writeFileSync(tempFilePath, pdfBuffer)

// 2. Carregar PDF com LangChain
const loader = new PDFLoader(tempFilePath)
const docs = await loader.load()  // Retorna Document[]

// 3. Dividir em chunks (opcional, jÃ¡ vem dividido por pÃ¡gina)
const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200
})
const chunks = await textSplitter.splitDocuments(docs)

// 4. Gerar embeddings
const embeddings = new OpenAIEmbeddings({ openAIApiKey: apiKey })
const vectors = await embeddings.embedDocuments(
  chunks.map(c => c.pageContent)
)

// 5. Limpar arquivo
unlinkSync(tempFilePath)
```

#### CaracterÃ­sticas
- **Aceita**: `string` (caminho de arquivo) ou `Blob`
- **NÃ£o aceita**: `Buffer` diretamente
- **Retorna**: `Document[]` do LangChain com metadados (pÃ¡gina, etc)
- **Requer**: Salvar arquivo em disco (`/tmp`)
- **I/O de disco**: 2 operaÃ§Ãµes (write + read)

#### Vantagens
- âœ… Tudo integrado no ecossistema LangChain
- âœ… Metadados automÃ¡ticos (nÃºmero da pÃ¡gina, etc)
- âœ… API consistente para mÃºltiplos formatos (PDF, Word, etc)
- âœ… Splitting por pÃ¡gina automÃ¡tico

#### Desvantagens
- âŒ NÃ£o aceita Buffer (requer arquivo fÃ­sico)
- âŒ I/O de disco desnecessÃ¡rio em Lambda
- âŒ Mais lento (operaÃ§Ãµes de escrita/leitura)
- âŒ Pode falhar se `/tmp` estiver cheio
- âŒ CÃ³digo mais verboso (salvar, ler, limpar)
- âŒ Bundle maior (PDFLoader + dependÃªncias)

---

### Abordagem 2: HÃ­brida (pdf-parse + LangChain) âœ… Atual

```typescript
import pdfParse from 'pdf-parse'
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter'
import { OpenAIEmbeddings } from '@langchain/openai'

// 1. Extrair texto com pdf-parse (direto do buffer)
const pdfData = await pdfParse(pdfBuffer, { max: 0 })
const text = pdfData.text

// 2. Dividir em chunks com LangChain
const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200
})
const chunks = await textSplitter.splitText(text)

// 3. Gerar embeddings com LangChain
const embeddings = new OpenAIEmbeddings({ openAIApiKey: apiKey })
const vectors = await embeddings.embedDocuments(chunks)
```

#### CaracterÃ­sticas
- **Aceita**: `Buffer` diretamente
- **NÃ£o precisa**: Salvar arquivo em disco
- **Retorna**: `string` simples com todo o texto
- **I/O de disco**: 0 operaÃ§Ãµes
- **Performance**: Mais rÃ¡pido (sem I/O)

#### Vantagens
- âœ… Aceita Buffer diretamente (perfeito para Lambda)
- âœ… Sem I/O de disco (mais rÃ¡pido)
- âœ… CÃ³digo mais simples (menos linhas)
- âœ… Mais controle sobre parsing (opÃ§Ãµes customizadas)
- âœ… Bundle menor (sÃ³ pdf-parse)
- âœ… Mesma qualidade de chunks e embeddings (usa LangChain)

#### Desvantagens
- âŒ Sem metadados automÃ¡ticos de pÃ¡gina
- âŒ Precisa gerenciar pdf-parse separadamente
- âŒ NÃ£o aproveita abstraÃ§Ã£o do LangChain para extraÃ§Ã£o

---

### ComparaÃ§Ã£o Lado a Lado

| Aspecto | LangChain 100% (PDFLoader) | HÃ­brida (pdf-parse + LangChain) |
|---------|----------------------------|----------------------------------|
| **ExtraÃ§Ã£o de texto** | PDFLoader (usa pdf-parse internamente) | pdf-parse direto |
| **Aceita Buffer?** | âŒ NÃ£o (precisa arquivo/Blob) | âœ… Sim |
| **I/O de disco** | âœ… Sim (write + read) | âŒ NÃ£o |
| **Performance** | Mais lento | Mais rÃ¡pido |
| **Linhas de cÃ³digo** | ~15 linhas | ~8 linhas |
| **Retorno** | `Document[]` (com metadados) | `string` simples |
| **Metadados** | AutomÃ¡tico (pÃ¡gina, etc) | Manual (se precisar) |
| **Complexidade** | Mais cÃ³digo (salvar/limpar) | Menos cÃ³digo |
| **Controle** | Limitado | Total sobre parsing |
| **Bundle size** | Maior (PDFLoader) | Menor (sÃ³ pdf-parse) |
| **Lambda-friendly** | âš ï¸ Depende de `/tmp` | âœ… Stateless |
| **Chunking** | LangChain âœ… | LangChain âœ… (igual) |
| **Embeddings** | LangChain âœ… | LangChain âœ… (igual) |

---

### Por Que Escolhemos a Abordagem HÃ­brida?

1. **Performance**: Sem I/O de disco = mais rÃ¡pido
2. **Simplicidade**: Menos cÃ³digo, menos complexidade
3. **Lambda-friendly**: NÃ£o depende de `/tmp`, 100% stateless
4. **Flexibilidade**: Controle total sobre parsing
5. **MantÃ©m LangChain onde importa**: Chunks e embeddings

#### O que usamos de cada biblioteca

```
pdf-parse:
  - ExtraÃ§Ã£o de texto do PDF
  - Aceita Buffer diretamente
  - Fornece metadados bÃ¡sicos (pÃ¡ginas, info)

LangChain:
  - RecursiveCharacterTextSplitter (chunking inteligente)
  - OpenAIEmbeddings (geraÃ§Ã£o de vetores)
  - NÃ£o usamos: PDFLoader (substituÃ­do por pdf-parse)
```

---

### Quando Usar Cada Abordagem?

#### Use LangChain 100% (PDFLoader) quando:
- Precisa de metadados automÃ¡ticos de pÃ¡gina
- Quer tudo integrado no ecossistema LangChain
- NÃ£o se importa com I/O de disco
- EstÃ¡ em ambiente com sistema de arquivos estÃ¡vel
- Processa mÃºltiplos formatos (PDF, Word, TXT)

#### Use Abordagem HÃ­brida (atual) quando:
- Performance Ã© importante
- EstÃ¡ em Lambda ou ambiente serverless
- Quer controle total sobre parsing
- Quer cÃ³digo mais simples
- NÃ£o precisa de metadados complexos de pÃ¡gina
- Quer bundle menor

---

### CÃ³digo de ReferÃªncia (ImplementaÃ§Ã£o Atual)

```typescript
// server/api/ingest.post.ts

// Decodificar PDF de base64
const pdfBuffer = Buffer.from(jsonBody.body, 'base64')

// 1. Extrair texto com pdf-parse (direto do buffer)
const pdfData = await pdfParse(pdfBuffer, { max: 0 })
const text = pdfData.text

// 2. Criar chunks com LangChain (AUTOMÃTICO)
const textSplitter = new RecursiveCharacterTextSplitter({ 
  chunkSize: 1000,
  chunkOverlap: 200
})
const chunks = await textSplitter.splitText(text)

// 3. Gerar embeddings com LangChain (AUTOMÃTICO)
const embeddings = new OpenAIEmbeddings({ openAIApiKey: config.openaiApiKey })
const vectors = await embeddings.embedDocuments(chunks)

// 4. Salvar no Qdrant
await qdrant.upsert(collectionName, { points: ... })
```

---

### ConclusÃ£o

A **Abordagem HÃ­brida** oferece o melhor dos dois mundos:
- ExtraÃ§Ã£o rÃ¡pida e eficiente com `pdf-parse`
- Chunking e embeddings robustos com `LangChain`
- CÃ³digo simples, rÃ¡pido e Lambda-friendly

VocÃª nÃ£o perde nada importante (chunking e embeddings sÃ£o idÃªnticos), apenas ganha performance e simplicidade.
